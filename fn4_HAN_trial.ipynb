{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.layers import Embedding, GRU, LSTM, Bidirectional, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, Callback, LambdaCallback, TensorBoard\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_final/reliable_final.pkl', 'rb') as infile:\n",
    "    reliable = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_final/unreliable_final.pkl', 'rb') as infile:\n",
    "    unreliable = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reliable['label'] = 0\n",
    "unreliable['label'] = 1\n",
    "data = reliable.append(unreliable).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "reliable = None\n",
    "unreliable = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 30  # max num words processed for each sentence\n",
    "max_sentences = 30  # max num sentences processed for each article \n",
    "max_vocab = 50000\n",
    "embedding_dim = 100  # size of pretrained word vectors\n",
    "attention_dim = 128  # num units in attention layer\n",
    "GRU_dim = 64  # num units in GRU layer, but it is bidirectional so outputs double this number\n",
    "batch_size = 128\n",
    "test_size = 0.2\n",
    "\n",
    "vector_dir = './embeddings'\n",
    "vector_file = 'glove.6B.100d.txt'\n",
    "model_dir = './model_output/glove_100'\n",
    "tb_logs = './tb_logs/glove_100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test sets. Test set will not be used in any way until model is trained.\n",
    "# The data splits created here will be saved \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['content'], data['label'], test_size=test_size,\n",
    "                                                    random_state=77, stratify=data['label'])\n",
    "data = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocab word index (dict) from training articles\n",
    "\n",
    "from collections import Counter \n",
    "words = Counter()\n",
    "for article in X_train:\n",
    "    for sentence in article:\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        words.update(sentence)\n",
    "    \n",
    "#with open('data_final/words.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(words, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size,\n",
    "                                                    random_state=77, stratify=y_train)\n",
    "\n",
    "#with open('aws/data/X_train.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(X_train, outfile)\n",
    "    \n",
    "#with open('aws/data/X_val.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(X_val, outfile)\n",
    "\n",
    "#with open('aws/data/X_test.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(X_test, outfile)\n",
    "\n",
    "#with open('aws/data/y_train.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(y_train, outfile)\n",
    "    \n",
    "#with open('aws/data/y_val.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(y_val, outfile)\n",
    "\n",
    "#with open('aws/data/y_test.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(y_test, outfile)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An embedding matrix must be constructed so that the embedding layer can look up and use the vector associated with each word. As an intermediary step, the words are first replaced with their respective indices in the word index. The embedding matrix uses these same indices for the lookup.\n",
    "\n",
    "Only the most common *n* words will be used from the word index. By pre-populating the data and embedding matrices with zeros, any words outside the *n*-word vocabulary or not found in the set of embedding vectors will be represented by a zero or vector of zeros. The GRU and attention layers support masking, which avoids having to process vectors of all zeros.\n",
    "\n",
    "Trial runs to ensure that the network is working properly and to determine learning rate bounds (to be used with a cyclic learning rate approach) will be conducted with a small subset of the training data comprising 10,000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ix, (word, _) in enumerate(words.most_common(max_vocab)):\n",
    "    word_index[word] = ix + 1 # The zero index is reserved for masking out-of-vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_matrix(data, max_sentences=max_sentences, max_words=max_words, max_vocab=max_vocab,\n",
    "                      word_index=word_index):\n",
    "    data_matrix = np.zeros((len(data), max_sentences, max_words), dtype='int32')\n",
    "    for i, article in enumerate(data):\n",
    "        for j, sentence in enumerate(article):\n",
    "            if j == max_sentences:\n",
    "                break\n",
    "            k = 0\n",
    "            for word in sentence:\n",
    "                if k == max_words:\n",
    "                    break\n",
    "                ix = word_index.get(word.lower())\n",
    "                if ix is not None and ix < max_vocab:\n",
    "                    data_matrix[i, j, k] = ix\n",
    "                k = k + 1\n",
    "    return data_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_X_train = X_train[X_train['label'] == 1].sample(4000, random_state=77).append(\n",
    "    X_train[X_train['label'] == 1].sample(4000, random_state=77))\n",
    ", sample_X_test, sample_y_train, sample_y_test = train_test_split(sample_data['content'],\n",
    "                                                    sample_data['label'], test_size=test_size,\n",
    "                                                    random_state=77, stratify=sample_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_train_matrix = create_data_matrix(sample_X_train)\n",
    "sample_test_matrix = create_data_matrix(sample_X_test)\n",
    "\n",
    "sample_y_train = np.asarray(to_categorical(sample_y_train))\n",
    "sample_y_test = np.asarray(to_categorical(sample_y_test))\n",
    "\n",
    "sample_data = shuffle(sample_data)  # For finding the learning rate, no validation set needed\n",
    "sample_targets = np.asarray(to_categorical(sample_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_embeddings(vector_dir=vector_dir, vector_file=vector_file):\n",
    "    embeddings = {}\n",
    "    with open(os.path.join(vector_dir, vector_file)) as vectors:\n",
    "        for line in vectors:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            weights = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = weights\n",
    "    return embeddings\n",
    "            \n",
    "embeddings = store_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(max_vocab=max_vocab, embeddings=embeddings, word_index=word_index,\n",
    "                            embedding_dim=embedding_dim):\n",
    "    embedding_matrix = np.zeros((max_vocab + 1, embedding_dim)) # max_vocab + 1 to account for 0 as masking index\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will remain all zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "            \n",
    "embedding_matrix = create_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct attention layer\n",
    "\n",
    "from tensorflow import matmul\n",
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    ''''''\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init_weights = initializers.get('glorot_normal')\n",
    "        self.init_bias = initializers.get('zeros')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init_weights((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init_bias((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init_weights((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):        \n",
    "        #uit = K.tile(K.expand_dims(self.W, axis=0), (K.shape(x)[0], 1, 1))\n",
    "        #uit = matmul(x, uit)\n",
    "        #uit = K.tanh(K.bias_add(uit, self.b))\n",
    "        #ait = K.dot(uit, self.u)\n",
    "        #ait = K.squeeze(ait, -1)\n",
    "\n",
    "        #ait = K.exp(ait)\n",
    "        \n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(attention_dim=attention_dim, GRU_dim=GRU_dim, drop=False, drop_pct=None,\n",
    "                embedding_matrix=embedding_matrix, embedding_dim=embedding_dim, word_index=word_index):\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                                input_length=max_words, trainable=False, mask_zero=True)\n",
    "\n",
    "    #  Layers for processing words in each sentence with attention; output is encoded sentence vector \n",
    "    sentence_input = Input(shape=(max_words,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    lstm_word = Bidirectional(GRU(GRU_dim, return_sequences=True))(embedded_sequences)\n",
    "    attn_word = HierarchicalAttentionNetwork(attention_dim)(lstm_word)\n",
    "    sentence_encoder = Model(sentence_input, attn_word)\n",
    "    \n",
    "    #  Layers for processing sentences in each article with attention; output is prediction\n",
    "    article_input = Input(shape=(max_sentences, max_words), dtype='int32')\n",
    "    article_encoder = TimeDistributed(sentence_encoder)(article_input)\n",
    "    lstm_sentence = Bidirectional(GRU(GRU_dim, return_sequences=True))(article_encoder)\n",
    "    attn_sentence = HierarchicalAttentionNetwork(attention_dim)(lstm_sentence)\n",
    "    #  The Adam optimizer also will be tried and can take a dropout layer\n",
    "    if drop:\n",
    "        drop_sentence = Dropout(drop_pct)(attn_sentence)\n",
    "        preds = Dense(2, activation='softmax')(drop_sentence)\n",
    "    else:\n",
    "        preds = Dense(2, activation='softmax')(attn_sentence)\n",
    "    \n",
    "    return Model(article_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create checkpoints to save information from each training epoch\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=model_dir+'weights.{epoch:02d}.hdf5')\n",
    "tb_checkpoint = TensorBoard(log_dir=tb_logs, histogram_freq=1, batch_size=128, write_graph=False, write_grads=True,\n",
    "                            write_images=True)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "if not os.path.exists(tb_logs):\n",
    "        os.makedirs(tb_logs, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network will be trained using a cyclic learning rate as proposed in the 2017 paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/pdf/1506.01186.pdf). This enables quicker convergence by helping to avoid getting stuck in local minima or saddle points on the loss surface. A [reference implementation](https://github.com/bckenstler/CLR) is available on Github, and a three-post walk-through on [pyimagesearch](https://www.pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/) includes code in more detail. The approach has two steps. The first uses a learning rate vs. loss graph to determine the minimum and maximum learning rates. A Keras callback then varies the learning rate back and forth between these two values during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_lr = 1e-8\n",
    "K.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "model = build_model()\n",
    "opt = SGD(lr=start_lr, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cyclic.rate_finder import LearningRateFinder\n",
    "find_lr = LearningRateFinder(model)\n",
    "\n",
    "find_lr_data = create_data_matrix(sample_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_lr.find(find_lr_data, sample_targets, start_lr=1e-8, end_lr=1., epochs=3, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_lr.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that effective learning (steady drop in loss) begins around $10^{-2.5}$, and\n",
    "learning becomes erratic and the loss quickly explodes above $10^{-1}$. So the learning rates will cycle between these two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from cyclic.rate_cycler import CyclicLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the min/max learning rates determined above, do test runs with the sample_train and sample_test data to determine the best choices for number of embedding, attention and GRU dimensions. Start with the smallest and see if increasing these dimensions results in significant performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clr = CyclicLR(epochs=4, num_samples=8000, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "model = build_model()\n",
    "opt = SGD(momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(sample_train_matrix, sample_y_train, validation_data=(sample_test_matrix, sample_y_test),\n",
    "          batch_size=batch_size, epochs=4, callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clr.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "model = build_model()\n",
    "opt = SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(sample_train_matrix, sample_y_train, validation_data=(sample_test_matrix, sample_y_test),\n",
    "          batch_size=batch_size, epochs=4, callbacks=[clr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nesterov momentum did not improve performance, but the algorithms should also be tried without the cyclic learning rate, after which the Adam optimizer will be tried. A first experiment with Adam showed excessive overfitting, so it will be tried with a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_trial(attention_dim=attention_dim, GRU_dim=GRU_dim, embedding_matrix=embedding_matrix, embedding_dim=embedding_dim,\n",
    "              momentum=0.9, batch_size=batch_size, epochs=4, cyclic=False, nesterov=False, adam=False, drop_pct=0.5,\n",
    "              X=sample_train_matrix, y=sample_y_train, X_val=sample_test_matrix, y_val=sample_y_test,\n",
    "              word_index=word_index, scale_factor=0.5, base_lr=10**(-2.5), max_lr=1e-1):\n",
    "    K.clear_session()\n",
    "    try:\n",
    "        del model\n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if adam is True:\n",
    "        opt = Adam()\n",
    "        model = build_model(attention_dim=attention_dim, GRU_dim=GRU_dim, embedding_matrix=embedding_matrix,\n",
    "                            embedding_dim=embedding_dim, word_index=word_index, drop=True, drop_pct=drop_pct)\n",
    "    else:\n",
    "        opt = SGD(momentum=momentum, nesterov=nesterov)\n",
    "        model = build_model(attention_dim=attention_dim, GRU_dim=GRU_dim, embedding_matrix=embedding_matrix,\n",
    "                            embedding_dim=embedding_dim, word_index=word_index)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "    \n",
    "    if cyclic is True:\n",
    "        clr = CyclicLR(epochs=epochs, num_samples=8000, batch_size=batch_size, scale_factor=scale_factor,\n",
    "                       base_lr=base_lr, max_lr=max_lr)\n",
    "        model.fit(X, y, validation_data=(sample_test_matrix, sample_y_test),\n",
    "                  batch_size=batch_size, epochs=epochs, callbacks=[clr])\n",
    "    else:\n",
    "        model.fit(X, y, validation_data=(sample_test_matrix, sample_y_test),\n",
    "                  batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial()  # SGD without cyclic learning rate or Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(nesterov=True)  # SGD without cyclic learning rate but with Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(adam=True)  # Adam optimizer with default 50% dropout in feed to prediction layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 50% dropout somewhat lessened the gap between the training and validation accuracy scores, both of which are significantly higher than with SGD optimization, but the model is still overfitting significantly. Even worse, the validation accuracy has decreased during the last epoch, while the training accuracy crossed 90%, leaving not much room for improvement. Reducing the GRU and/or attention dimensions could help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(attention_dim=64, adam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=32, attention_dim=64, adam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=16, attention_dim=32, adam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=16, attention_dim=16, adam=True, drop_pct=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=16, attention_dim=8, adam=True, drop_pct=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=8, attention_dim=8, adam=True, drop_pct=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the GRU and attention dimensions as low as 8 and the dropout up to 70%, it can be seen that the training accuracy races ahead of the validation accuracy by the fourth epoch. Running it on the full dataset of 600,000+ articles would only make matters worse, although it is possible that good results could be obtained with a very few training epochs. The best performing combination for more extensive training without overfitting was the first, SGD with cyclic learning and no Nesterov momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(momentum=0.8, cyclic=True)  # Reduced momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing momentum does not give any performance gain, so keep it at 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=100, attention_dim=200, cyclic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the attention and GRU dimensions does not improve performance. Can these be reduced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=50, attention_dim=100, cyclic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=64, attention_dim=32, cyclic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that increasing the GRU dimension while decreasing the attention dimension improves performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=128, attention_dim=32, cyclic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the computation of the hidden state by the GRU seems the critival component, increasing the size of the word embedding and/or the vocabulary could improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_200 = store_embeddings(vector_file='glove.6B.200d.txt')\n",
    "embedding_matrix_200 = create_embedding_matrix(embeddings=embeddings_200, embedding_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=128, attention_dim=32, embedding_matrix=embedding_matrix_200, embedding_dim=200, cyclic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200, embedding_dim=200, cyclic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index_80k = {}\n",
    "for ix, (word, _) in enumerate(words.most_common(80000)):\n",
    "    word_index_80k[word] = ix + 1 # The zero index is reserved for masking out-of-vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_train_matrix_80k = create_data_matrix(sample_X_train, max_vocab=80000, word_index=word_index_80k)\n",
    "sample_test_matrix_80k = create_data_matrix(sample_X_test, max_vocab=80000, word_index=word_index_80k)\n",
    "\n",
    "embedding_matrix_100_80k = create_embedding_matrix(max_vocab=80000, word_index=word_index_80k)\n",
    "embedding_matrix_200_80k = create_embedding_matrix(max_vocab=80000, embeddings=embeddings_200,\n",
    "                                                   word_index=word_index_80k, embedding_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=128, attention_dim=32, embedding_matrix=embedding_matrix_100_80k, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_100_80k, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=128, attention_dim=32, embedding_matrix=embedding_matrix_200_80k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200_80k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Raise the base learning rate a little and increase the scale factor, so the max rate decreases more quickly\n",
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200_80k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k, scale_factor=0.33,\n",
    "          base_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index_150k = {}\n",
    "for ix, (word, _) in enumerate(words.most_common(150000)):\n",
    "    word_index_150k[word] = ix + 1 # The zero index is reserved for masking out-of-vocab words\n",
    "    \n",
    "sample_train_matrix_150k = create_data_matrix(sample_X_train, max_vocab=150000, word_index=word_index_150k)\n",
    "sample_test_matrix_150k = create_data_matrix(sample_X_test, max_vocab=150000, word_index=word_index_150k)\n",
    "\n",
    "embedding_matrix_100_150k = create_embedding_matrix(max_vocab=150000, word_index=word_index_150k)\n",
    "embedding_matrix_200_150k = create_embedding_matrix(max_vocab=150000, embeddings=embeddings_200,\n",
    "                                                    word_index=word_index_150k, embedding_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=128, attention_dim=32, embedding_matrix=embedding_matrix_100_150k, cyclic=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_100_150k, cyclic=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=128, attention_dim=32, embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 200-length vector embeddings performed better across the board. With the 150,000-word vocabulary, the network with GRU/attention dimensions of 128/32 performed almost as well as the 200/100 network, but the accuracy was increasing faster at the end of the four epochs and it took 3/5 of the training time. With the 80,000-word vocabulary, the 200/100 network performed better, after backtracking on the second epoch. Both of these will be tried with the full dataset. A minimum improvement of It is to be noted, however, that the accuracies obtained after four epochs are significantly lower than that obtained with tf-idf logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(bin(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}