{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.layers import Embedding, GRU, LSTM, Bidirectional, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, Callback, LambdaCallback, TensorBoard\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_final/reliable_final.pkl', 'rb') as infile:\n",
    "    reliable = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_final/unreliable_final.pkl', 'rb') as infile:\n",
    "    unreliable = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reliable['label'] = 0\n",
    "unreliable['label'] = 1\n",
    "data = reliable.append(unreliable).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reliable = None\n",
    "unreliable = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 30  # max num words processed for each sentence\n",
    "max_sentences = 30  # max num sentences processed for each article \n",
    "max_vocab = 50000\n",
    "embedding_dim = 100  # size of pretrained word vectors\n",
    "attention_dim = 64  # num units in attention layer\n",
    "GRU_dim = 128  # num units in GRU layer, but it is bidirectional so outputs double this number\n",
    "batch_size = 64\n",
    "test_size = 0.2\n",
    "\n",
    "vector_dir = './embeddings'\n",
    "vector_file = 'glove.6B.100d.txt'\n",
    "model_dir = './model_output/glove_100'\n",
    "tb_logs = './tb_logs/glove_100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test sets. Test set will not be used in any way until model is trained.\n",
    "# The data splits created here will be saved \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['content'], data['label'], test_size=test_size,\n",
    "                                                    random_state=77, stratify=data['label'])\n",
    "data = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocab word index (dict) from training articles\n",
    "\n",
    "from collections import Counter \n",
    "words = Counter()\n",
    "for article in X_train:\n",
    "    for sentence in article:\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        words.update(sentence)\n",
    "    \n",
    "#with open('data_final/words.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(words, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size,\n",
    "                                                    random_state=77, stratify=y_train)\n",
    "\n",
    "#with open('aws/data/X_train.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(X_train, outfile)\n",
    "    \n",
    "#with open('aws/data/X_val.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(X_val, outfile)\n",
    "\n",
    "#with open('aws/data/X_test.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(X_test, outfile)\n",
    "\n",
    "#with open('aws/data/y_train.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(y_train, outfile)\n",
    "    \n",
    "#with open('aws/data/y_val.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(y_val, outfile)\n",
    "\n",
    "#with open('aws/data/y_test.pkl', 'wb') as outfile:\n",
    "#    pickle.dump(y_test, outfile)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An embedding matrix must be constructed so that the embedding layer can look up and use the vector associated with each word. As an intermediary step, the words are first replaced with their respective indices in the word index. The embedding matrix uses these same indices for the lookup.\n",
    "\n",
    "Only the most common *n* words will be used from the word index. By pre-populating the data and embedding matrices with zeros, any words outside the *n*-word vocabulary or not found in the set of embedding vectors will be represented by a zero or vector of zeros. The GRU and attention layers support masking, which avoids having to process vectors of all zeros.\n",
    "\n",
    "Trial runs to ensure that the network is working properly and to determine learning rate bounds (to be used with a cyclic learning rate approach) will be conducted with a small subset of the training data comprising 10,000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "for ix, (word, _) in enumerate(words.most_common(max_vocab)):\n",
    "    word_index[word] = ix + 1 # The zero index is reserved for masking out-of-vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_matrix(data, max_sentences=max_sentences, max_words=max_words, max_vocab=max_vocab,\n",
    "                      word_index=word_index):\n",
    "    data_matrix = np.zeros((len(data), max_sentences, max_words), dtype='int32')\n",
    "    for i, article in enumerate(data):\n",
    "        for j, sentence in enumerate(article):\n",
    "            if j == max_sentences:\n",
    "                break\n",
    "            k = 0\n",
    "            for word in sentence:\n",
    "                if k == max_words:\n",
    "                    break\n",
    "                ix = word_index.get(word.lower())\n",
    "                if ix is not None and ix < max_vocab:\n",
    "                    data_matrix[i, j, k] = ix\n",
    "                k = k + 1\n",
    "    return data_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_X = X_train.iloc[0:10000]\n",
    "sample_y = y_train.iloc[0:10000]\n",
    "sample_X_train, sample_X_test, sample_y_train, sample_y_test = train_test_split(sample_X, sample_y,\n",
    "                                                    test_size=test_size, random_state=77, stratify=sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_matrix = create_data_matrix(sample_X_train)\n",
    "sample_test_matrix = create_data_matrix(sample_X_test)\n",
    "\n",
    "sample_y_train = np.asarray(to_categorical(sample_y_train))\n",
    "sample_y_test = np.asarray(to_categorical(sample_y_test))\n",
    "\n",
    "# No validation will be needed for finding the learning rate\n",
    "find_lr_data = create_data_matrix(sample_X)\n",
    "find_lr_targets = np.asarray(to_categorical(sample_y))\n",
    "\n",
    "X_train = None\n",
    "y_train = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_embeddings(vector_dir=vector_dir, vector_file=vector_file):\n",
    "    embeddings = {}\n",
    "    with open(os.path.join(vector_dir, vector_file)) as vectors:\n",
    "        for line in vectors:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            weights = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = weights\n",
    "    return embeddings\n",
    "            \n",
    "embeddings = store_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(max_vocab=max_vocab, embeddings=embeddings, word_index=word_index,\n",
    "                            embedding_dim=embedding_dim):\n",
    "    embedding_matrix = np.zeros((max_vocab + 1, embedding_dim)) # max_vocab + 1 to account for 0 as masking index\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will remain all zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "            \n",
    "embedding_matrix = create_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct attention layer\n",
    "\n",
    "from tensorflow import matmul\n",
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    ''''''\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init_weights = initializers.get('glorot_normal')\n",
    "        self.init_bias = initializers.get('zeros')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init_weights((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init_bias((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init_weights((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):        \n",
    "        #uit = K.tile(K.expand_dims(self.W, axis=0), (K.shape(x)[0], 1, 1))\n",
    "        #uit = matmul(x, uit)\n",
    "        #uit = K.tanh(K.bias_add(uit, self.b))\n",
    "        #ait = K.dot(uit, self.u)\n",
    "        #ait = K.squeeze(ait, -1)\n",
    "\n",
    "        #ait = K.exp(ait)\n",
    "        \n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(attention_dim=attention_dim, GRU_dim=GRU_dim, drop=False, drop_pct=None,\n",
    "                embedding_matrix=embedding_matrix, embedding_dim=embedding_dim, word_index=word_index):\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                                input_length=max_words, trainable=False, mask_zero=True)\n",
    "\n",
    "    #  Layers for processing words in each sentence with attention; output is encoded sentence vector \n",
    "    sentence_input = Input(shape=(max_words,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    lstm_word = Bidirectional(GRU(GRU_dim, return_sequences=True))(embedded_sequences)\n",
    "    attn_word = HierarchicalAttentionNetwork(attention_dim)(lstm_word)\n",
    "    sentence_encoder = Model(sentence_input, attn_word)\n",
    "    \n",
    "    #  Layers for processing sentences in each article with attention; output is prediction\n",
    "    article_input = Input(shape=(max_sentences, max_words), dtype='int32')\n",
    "    article_encoder = TimeDistributed(sentence_encoder)(article_input)\n",
    "    lstm_sentence = Bidirectional(GRU(GRU_dim, return_sequences=True))(article_encoder)\n",
    "    attn_sentence = HierarchicalAttentionNetwork(attention_dim)(lstm_sentence)\n",
    "    #  The Adam optimizer also will be tried and can take a dropout layer\n",
    "    if drop:\n",
    "        drop_sentence = Dropout(drop_pct)(attn_sentence)\n",
    "        preds = Dense(2, activation='softmax')(drop_sentence)\n",
    "    else:\n",
    "        preds = Dense(2, activation='softmax')(attn_sentence)\n",
    "    \n",
    "    return Model(article_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create checkpoints to save information from each training epoch\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=model_dir+'weights.{epoch:02d}.hdf5')\n",
    "tb_checkpoint = TensorBoard(log_dir=tb_logs, histogram_freq=1, batch_size=128, write_graph=False, write_grads=True,\n",
    "                            write_images=True)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "if not os.path.exists(tb_logs):\n",
    "        os.makedirs(tb_logs, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network will be trained using a cyclic learning rate as proposed in the 2017 paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/pdf/1506.01186.pdf). This enables quicker convergence by helping to avoid getting stuck in local minima or saddle points on the loss surface. A [reference implementation](https://github.com/bckenstler/CLR) is available on Github, and a three-post walk-through on [pyimagesearch](https://www.pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/) includes code in more detail. The approach has two steps. The first uses a learning rate vs. loss graph to determine the minimum and maximum learning rates. A Keras callback then varies the learning rate back and forth between these two values during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_lr = 1e-8\n",
    "K.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "model = build_model()\n",
    "opt = SGD(lr=start_lr, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cyclic.rate_finder import LearningRateFinder\n",
    "find_lr = LearningRateFinder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 389s 39ms/step - loss: 0.6955 - acc: 0.4459\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 383s 38ms/step - loss: 0.6943 - acc: 0.4707\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 385s 39ms/step - loss: 0.7367 - acc: 0.5937\n"
     ]
    }
   ],
   "source": [
    "# Use a smaller batch size for a more fine-grained tuning of the learning rate\n",
    "find_lr.find(find_lr_data, find_lr_targets, start_lr=1e-8, end_lr=1., epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAFSCAYAAABPDYNlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8XHW9//HXJzPZ96RJt6QrXaEblLbsOxTZVBBB0Qte\nLnK9CupVr+hF+en1XhUVFVHEDQU3RBTEQtn3rfu+pXvSNmuzrzPz/f1xJuk0Sdt0meSkfT8fj3kk\n53u+55zPTJd88l3NOYeIiIiI+FfCQAcgIiIiIgenhE1ERETE55SwiYiIiPicEjYRERERn1PCJiIi\nIuJzSthEREREfE4Jm8hxysxuNjNnZicNdCz9Lea9jxnoWA7EzMZEY7x5oGM5XGZ2vpndY2b6GSLS\nT/SPTURkYOwGzgD+OdCBHIHzga+jnyEi/SY40AGIiBwPzCwRCLk+rkbunGsD3olvVH1jZgHAnHOh\ngY5FRHqn345ETnBmdpOZrTCzVjOrMrNHzGx4tzofMbNlZtZoZvVmtsrMPhlz/nQze97Mqs2sxcy2\nmNlPD/LMYWYWMrM7ejn3JTPrMLOC6PFlZvaWmdVFn7/BzL52hO/1tm7v9VdmltetzqfN7G0zqzGz\nWjN7x8yu6FanszvzU2b2XTPbBbQBOTHdsfPM7PfRz2uXmf3YzFJ6ucfNMWUPm1mpmc0ys9fNrNnM\nNpnZ7b28l4ujfyatZlZiZrdGr9/Wh8/Bmdm3zOzLZrYVaAemmVmKmd1nZqujn/UeM/uHmU2OufYe\nvNY1gI7ovVzM+TQz+46ZbTWz9ujXr6r7VOToqIVN5ARmZrcBPwf+DNwFjAD+F5hrZqc65xrN7Gzg\nUeDHwBfxftGbDORE75EBLATeA24GGoAxwJkHeq5zbo+ZvQDcFL1vrI8BzzrnKs1sHPAU8DjwDbzE\nYgIw7gje67eB/4x5HyOB/wFOMbMznXPhaNWxwMPAZiAAXAU8bWaXO+ee7XbbrwKLgNuidVtjzj0C\n/BH4IF7X5z3AXvYlOweSBfwB+CHee74F+JmZbXDOvRx9L1PxulLfA24AkoC7gWwg0qcPxPuz2gJ8\nAWgCdgHJ0ef/H1AG5AKfAt42synOuT3AL4Ei4F+Bs4HOzw0zC+L9XZgKfBNYBcyLxpaH9/mLyJFw\nzumll17H4QvvB7IDTjrA+QBQDrzcrfzs6HV3RI+/ANQc5Dmzo/WnH2Z8H41eNymmbGa07Pro8XXR\n46wjfO9josdj8BKLr3Wrd1a03vsPcJ8EvF9snwOejCkfE71uKV5XYm/P/n/dyp8GNvZyj5tjyh6O\nll0QU5YMVAMPxZT9AagE0mLKhuMljNv68Pk4vAQt9RD1AkAaXhL+uZjye6L3CHar/7Fo+bndyr+K\nl2wXDvS/C730GqwvNVGLnLgmAYXA72MLnXNvANuB86JFi4BcM3vUzK40s5xu99kE1AI/j3avFvfx\n+X8DGvF+yHf6GFCH16oGsBzoAP5kZteZWWEf793dJXjJ1+/NLNj5At7FS0bO7axoZqeZ2dNmVg6E\nos+/BO/z6u7vzrkDjVnrPplgFTCqD7E2u2hLGnSNddvY7dp5wALnXHNMvd3AW324f6dnnXMt3QvN\n7Hoze9fMavHefxOQQe/vv7v5eH933ur2OT8HJEbjFpEjoIRN5MTVOXZrdy/n9nSed869CnwIKMZL\nsirN7AUzmx49XwdcgNdi81NgR3QM1LUHe3g02fgr8FHzBIAbgb8451qjdUqAy/D+r3oE2BMdU3be\nge57AJ2JXgleAhb7ygTyAaLJ5ovR9/4ZvG7d04FngRR66u2z61TT7bgNr7XsUPb2UtbW7fnDgYpe\n6pX34f6desRuZlfhdY+vAz4CzMV7/5X0/v67KwRG0/Mzfi96Pv8w4hORGBrDJnLi6kwohvVybhiw\npPPAOfc48Hh0vNr5wHeAZ82syDkXcc4tB66NtqbMxhsP95iZzXDOrT5IDI8A/4LXDZuKl4g8Elsh\n2tr0spkl43VhfgP4p5mNcc5V9fG9Vke/XkrvCVHn+fl448Cud86Vdp40s7QD3LdPM0LjYDf7ktBY\nQw/jHr3FfgNQ4py7ubPAvNmveb3U7U01sBW4/gDntx1GfCISQwmbyIlrA16LzA3ArzoLzexMvFaS\n73e/wDnXiDcAfxzwI7wWk8qY8yHgHTO7G7gamAIcLGF7GSjF6wpNxfuB/npvFaNdgy9Fk8Yn8SYH\n9DVhex5vMP4o59zzB6nXmZh1dBaY2US8RLG01ysGxjvA+8wsrbNb1LyZvWdx8Fa/Q0nD6waN9TG8\nsWyx2qJfU/G6lDs9C1wLNDrn1h9FHCLSjRI2kePffDPb062szjn3fHR5jJ+b2aN4M0FHAt/CG5f2\nawAz+wZey83LeN2eRcAdwHLnzeS8Em+W5N/xWlfSo+cbgLcPFphzLmJmvwc+iTfG6b7YMWHR5SzO\nBRYAO4EheK13uzh4Itj9OZvN7DvAT8xsEvAq3gD9Yrzxab+MtuS9gJew/M7Mvo/X4vf/gB34awjJ\n/+BNyFhoZt/D62q9Gy8B7+ss0d48C7zfzO7DmyQxG69ruLZbvbXRr/9pZs8AYefcYrzxkLcAL0Y/\nvxV4M1jH4yXw748ddycifaeETeT4d38vZWuAU5xzD5lZM94yF0/iTQJYAHzJOdcUrfsuXgJ2H17X\nWAXeIPK7o+c3AS3R4+F4idoi4JLYbsWDeAT4r5jvY60ALsdbZqIQrxv3DeCjvQ2YPxjn3FfMbB3w\nH9GXw0sCX4y+B5xza8zso3jdrk/hLe3xZbyu0vMP53nx5Jxba97acPcCj+EtwfEdvDjHHMWtf4GX\nxH4CL4lehLesyd+61Xsab7zip4CvAYY3W7bDzC7D+8xuw2sFbcL7HP+JN1NURI6AHXiCk4iIDBbR\nruIS4J/OuX8d6HhE5NhSC5uIyCBkZvfjLeOxC2/B4zvxFrr90UDGJSLxoYRNRGRwSsHrBh2K19X4\nHnCxc27lgEYlInGhLlERERERn/PTrCcRERER6cVx1SU6ZMgQN2bMmIEOQ0REROSQlixZUuWcK+hL\n3eMqYRszZgyLFy8e6DBEREREDsnMtve1rrpERURERHxOCZuIiIiIzylhExEREfE5JWwiIiIiPqeE\nTURERMTn4pqwmdl8M9tgZiVm9uVezn/RzJZHX6vNLGxmeTHnA2a2zMyejmecIiIiIn4Wt4TNzALA\nA8DlwFTgRjObGlvHOXevc26mc24mcBfwqnOuJqbKncC6eMUoIiIiMhjEs4VtDlDinNvinGsH/gRc\nc5D6NwJ/7DwwsyLgCuCXcYxRRERExPfimbCNBHbGHJdGy3owszRgPvDXmOIfAl8CIgd7iJndZmaL\nzWxxZWXl0UUsIiIi4kN+mXRwFfBmZ3eomV0JVDjnlhzqQufcQ8652c652QUFfdrdQURERKRXu+ta\nWF1WN9Bh9BDPhK0MKI45LoqW9eYGYrpDgbOAq81sG15X6oVm9mg8ghQRERHp9Js3t/GhB98e6DB6\niGfCtgiYYGZjzSwJLyl7qnslM8sGzgOe7Cxzzt3lnCtyzo2JXveSc+6mOMYqIiIiQiTiMBvoKHqK\n2+bvzrmQmX0aWAgEgF8759aY2e3R8w9Gq34AeM451xSvWERERET6wgEJPszY4pawATjnFgALupU9\n2O34YeDhg9zjFeCVYx6ciIiISDcR588WNr9MOhAREREZcM75s4VNCZuIiIhIVMQ5EvyXrylhExER\nEenkdYn6L2NTwiYiIiIS5XWJDnQUPSlhExEREYmKONTCJiIiIuJnTmPYRERERPzNm3Tgv4xNCZuI\niIhIlHPgv3RNCZuIiIhIF41hExEREfE55xwJPsyOfBiSiIiIyMDQGDYRERERn4toDJuIiIiIvzm0\nl6iIiIiIr3lbUw10FD0pYRMRERGJchrDJiIiIuJvkYi6REVERER8zaEuURERERFf08K5IiIiIj6n\nzd9FREREfC7iNIZNRERExNciamETERER8TfnwI+zDpSwiYiIiESphU1ERETE55zGsImIiIj4m1rY\nRERERHzOaR02EREREX+LOIf/0jUlbCIiIiJdNIZNRERExOcizpHgw+zIhyGJiIiIDAxv0oFa2ERE\nRER8yw10AAeghE1EREQkSnuJioiIiPic0zpsIiIiIv6mMWwiIiIiPqeFc0VERER8LuLAh/maEjYR\nERGRThrDJiIiIuJzGsMmIiIi4nMn5LIeZjbfzDaYWYmZfbmX8180s+XR12ozC5tZnpkVm9nLZrbW\nzNaY2Z3xjFNEREQEvC5RH+Zr8UvYzCwAPABcDkwFbjSzqbF1nHP3OudmOudmAncBrzrnaoAQ8J/O\nuanAPOA/ul8rIiIicqydiLNE5wAlzrktzrl24E/ANQepfyPwRwDn3G7n3NLo9w3AOmBkHGMVERER\niY5hG+goeopnwjYS2BlzXMoBki4zSwPmA3/t5dwYYBbw7gGuvc3MFpvZ4srKyqMMWURERE5kJ+QY\ntsNwFfBmtDu0i5ll4CVxn3XO1fd2oXPuIefcbOfc7IKCgn4IVURERI5XjhNsDBtQBhTHHBdFy3pz\nA9Hu0E5mloiXrP3eOfdEXCIUERERiRGJnHgtbIuACWY21syS8JKyp7pXMrNs4DzgyZgyA34FrHPO\n/SCOMYqIiIh0cc7hv3Qtjgmbcy4EfBpYiDdp4DHn3Bozu93Mbo+p+gHgOedcU0zZWcDHgAtjlv14\nX7xiFREREQH/jmELxvPmzrkFwIJuZQ92O34YeLhb2RvgywRXREREjmMR50jwywj/GD4MSURERGRg\nOE68ddhEREREBhVt/i4iIiLicxEH5sNRWUrYRERERKJOxJ0ORERERAaVE3EvUREREZFBxWthU8Im\nIiIi4lteC9tAR9GTEjYRERGRKI1hExEREfE5dYmKiIiI+JwmHYiIiIj4nHOoS1RERETEzyLOadKB\niIiIiJ9pDJuIiIiIz2nzdxEREREfc85pDJuIiIiInznnfVWXqIiIiIhPRaIZm//SNSVsIiIiIgBE\nOlvYfNgnqoRNREREBHBEW9j8l68pYRMREREBjWETERER8b3OMWw+7BFVwiYiIiIC+8awmQ+nHShh\nExEREcFbhw00hk1ERETEtyIawyYiIiLib05j2ERERET8rWsMm1rYRERERPxJs0RFREREfM6phU1E\nRETE36oa2wBNOhARERHxrct/9DqgLlERERER32tuDw90CD0oYRMRERGJsXpX3UCH0IMSNhERERFg\n8rBMAG4+c8zABtILJWwiIiIiQGZKkDPG5TO9KGegQ+lBCZuIiIgIEI44An6ccYASNhEREREAwg4S\nlLCJiIiI+Fck4gj4M19TwiYiIiICEIo4Agn+TI3iGpWZzTezDWZWYmZf7uX8F81sefS12szCZpbX\nl2tFREREjqVIxBHwZ74Wv4TNzALAA8DlwFTgRjObGlvHOXevc26mc24mcBfwqnOupi/XioiIiBxL\nYXdiTjqYA5Q457Y459qBPwHXHKT+jcAfj/BaERERkaMSiThf7iMK8U3YRgI7Y45Lo2U9mFkaMB/4\n6xFce5uZLTazxZWVlUcdtIiIiJyYTtQWtsNxFfCmc67mcC90zj3knJvtnJtdUFAQh9BERETkRBCO\nOAInYAtbGVAcc1wULevNDezrDj3ca0VERESO2om6cO4iYIKZjTWzJLyk7KnulcwsGzgPePJwrxUR\nERE5VvycsAXjdWPnXMjMPg0sBALAr51za8zs9uj5B6NVPwA855xrOtS18YpVREREJOKcb3c6iFvC\nBuCcWwAs6Fb2YLfjh4GH+3KtiIiISLycqGPYxGf2NrXz3Jo97KhuHuhQREREfOeE7BIV/2hpD/ON\np9fyxNJS2kIRAM4+aQg3zRvF/FOGD3B0A6+htYOHXtvC2l31XDljONfMGOnbJnEREYmfiEMJ2/Eg\nFI7wt2VlzBuXT3Fe2kCHc0hPr9zFnxft5PVNVSQY3DhnFFdMH87ibXv586Kd3P7oUi47eSg3zRvN\nvHH5JPp1P44j4Jzr0z+8cMTxyUeW8NbmagBeXF/B/S+WMK0om4+fMYbTRuf26Xkd4chRf36vb6rk\nB89v5NRRuYzOT+P9s0aSHEwgORjoquOcw3zaXA9Q3dhGVWM76ckBhmWlUFbbwsvrKzh5ZDanj8kb\n6PBERA4qFIkoYTse/HVpKf/111UA/O4Tczh3on/Wfdte3URGcpA3SqoA+O1b21i6o5ahWckMy0rh\nv6+cwpXTRwBw5vghfOr88Xzz6bX8cdFOFq4pZ0hGMrecNYZPnT/+gAlBXUsH726pZsrwrH5JWJ1z\nPLl8Fy0dYc6fVEB2aiKpiQHMjLqWDjKSgzS3h/jFa1s4fWweZ44fwp76VhZtreEnL5dQ2dDG3VdO\n5brTigBoagtx95OrSU0McNWMEcwbl8+PXtzEW5ur+e5107lq+gh++/Y2fv7qZl5aV8GL6yr4xjUn\nk5YU5Ixx+WSnJe4X2+66VoZkJPPbt7Zx78INfPj0Yi6fNoyZxTkkBRJYWVbHtJHZXYncku01TBme\nRVqS989u+c5aHlu8k6RAAkMykrjvhU1kpyaybEctAF97cg1zxubx4E2n8fBb2yitaeaZ1Xto6Qjz\nHxeM5/OXTOqX/1jaQxEWb69hc0UjmyoauWrGCEbnp5FgRklFI8OzU/j7sl38edEOdte34px3XYJ5\nv612SksKcMPpo7hwciER5whHHOdMGELwOPpF4VgLhSO0hSKkJwdxztHaESE1KXDoC3vR2hHm/xas\n450tNcwszuGrV04hMzno618ARPpbJIJvdzow59yhaw0Ss2fPdosXL47LvZ1zXP6j1ymvbyUpmMBJ\nhRn8/tZ5cXnW4XphbTm3/m7/952dmsitZ4/lUxecdNAf6jtrmnl8SSnPrt7DhvIGLphUQE1TO+ML\nMjhjfD5TR2SRnhTk848tZ2k0kUgwmDM2j4/NG8PZJw3ZL5HpLnY8wAtry3lvWw23nDWG/PRkkoIJ\nhCOOBAMzY2dNM69vquIfK3bREY6QnZrIi+sr9rufGSQHE2jtiJCfnkRacoCdNS0AjMxJZU99K+GI\nIy0pwIicVEoqGjlvYgGJgQQ2VzaytaprMjLzxuXx7tYarj21iHuvm77fD649da185BfvsCVaPysl\nSH5GMimJAYZkJLGpvJE99a37xRZMMELR99OZqMwozqGtI8z6PQ0AnDk+n2HZKazYWcvmyqb9rj93\nYgEPfGQWHWHHqrI6bn9kCS0d4a7zBZnJVDa0dR1PGZ7F5GGZ7G1u57Zzx3Hm+CGA93e1dK/3mSQH\nE6hv7WBlaR0vrq/g8lOG8b5Thvepyzcccdy7cAN/X1bW9V67J2Gxzp9UwMShmUwdnkVTe4iyvS1k\npiRy0ZRCvvvsel5YV9HjmolDM/j2tdOZVZwz6BOHyoY2zGBIRnKf6i/ZXsOzq/fwzpYarp4xglvP\nGYuZEY44FqzajRn84rUtbChvYMrwLEoqGmloDXHWSflcPWNEtAXWS95Wldbx1uYq3t1aQ8Q5zp1Q\nwO66Ft4sqeaciUNITQzw+qYqlmzfu18MEwoz+OjcURTlpnHa6FzW7a7njZIqdtQ0c/WMEVw8ZaiG\nB8gJZfxXFnD7eeP44mWT++V5ZrbEOTe7T3WVsPWNc443SqpwDlaW1vK95zbyt0+dSUNriDlj80hJ\nPLLfersLRxwGB/1PsqG1g2dW7aGkspHM5CA/f20LbaEwU0dkc9X04RRmpTBvXB6FmSl9fq5zjn/7\n3RLe3VpNelKwRzKSnhTg/bNGcvGUoSzZvpenVuxiR00z6UkBHrl1Lku376WmqZ2X1ldQ0dDGR+aM\norqpjT8v2sm4ggxaO8JdSQRAUiCBmcU5LC+tZUh6Ep84eyyPvLOd7d0mRHx4djE56YmUlDcya1QO\nbaEIrR1hUhID7KhpZktlE3deNIGa5nZ+/+4OJg3N4BNnj2V0XjoR57jriVUsWLWbUMRRnJfKPVed\nzPiCDO5duIFF22o4eUQWP/3oab22WrSFwqzYWUd7KMK9z22gqqGNxrYQdS0dTC/K5rKTh/HPlbt5\n37Rh/Nu542hsDbGytI6X1lcQcY63N1d3JXyXnzKMDeUNbIkmaXPG5jFxaAZ3XDiBpvYwZXtbOGN8\n/n7JdSTiePTd7bxVUs1H543inAkFRCKOhrYQC1bt5n+eXktSMIGmtjDt4QifOGssTW0h/rx4365u\nZtD9n/hH547ia1dNJTkYYFN5A1uqmjh3QgG1Le1sq2rmJy9vYnxBBsnBBH7x+layUoJ845pTmFaU\nzZD0ZB5fWkp5fSvOOSYOzaQ9HGFUXhrnTDh4i3NFfSt7mztYs6uOIRnJbCxv4KevbKamqZ30pABX\nzxzJVTOGc8a4/H5P3hrbQuxtaj9oy3FlQxtLttewubKJ5GACze1hVpbWUtXYzqi8NJ5euYtgIIF/\nP288H5k7iqFZ+//729vUTlIwgdTEAPe/VMJ9L2wkkGCkJgZobAuRlRKkLRShIDO5699KWlKACyYV\n8s9VuxmRncLVM0fyxNJSKhraGJ2fRiDBKNvb0jU2tTgvlcRAQtffs1hDMpL50mWTuPTkoWyubOLF\ndeUsXLOnxy8OgQQjmGC0hSKcO7GAK6cN56wJQxiZk3q0H7OI74358j+548KT+Pylk/rleUrY4qyh\ntYMLv/9qV2vHtacW8YXLJjI8u2//oS3fWcv26iaunD6CQIJRUd/KUyt28bdlZWypbCItKcCdF09g\n2shs9ja38/zaCp5cXkZ6cpDkYAIV9W20hyNd95tRnMP3PzSDkwozjvq9df59qGho44mlZdy7cD3T\ni3L47nXTmTg0s6teRzjCaxsr+fxjK6hr6egqH5aVQnmD1y2WFEjgzJPySQokEIo4zptYwJgh6Tz4\nymaa20O0hx2zRuWwaGsNmyoaSUsKcPeVU7ly+nCa28OkJgXISjlw611f7alrJSct8Zgk1a0dYTrC\nETL7GFdjW4g1ZXXMGZvX1ZVb3djG2CHpR52UdI5na+0I8++PLuHlDfv20j1jXD4zinNIMBg7JJ3U\npAAXTi7kO8+s57dvb2dkTiptoTBVje0HfcY1M0fwww/PjFsCVVbbwh/e3c62qmZe3lBBc3uYq2aM\n4Mrpw3lm1W4A5o7L50OnFbFzbwsGrCqrY0dNM/PG5TF5WBbpyX0f2bGqtI7hOSnkpyfR2hFhR00z\nv3h9C48vKQVgdH4aRbmp3H7eeE4fk8dfl5ayqrSOVWV1rNlV3+N+mclBGtpCJAcTuH52MZsqGnhn\nSw3ZqYnMLM6hobWDq2aMIMGMbz69FoBgwGjtiPDBWSP5nw+cQkowwK/f3MrynbUMyUimpKKRa2aO\nYHR+OiNyUijKTWPd7nryM5IozEzBOcdL6yv42SubyUtPYnh2CicVZjB3XD7jCzIIJBile5tJTwqS\nnZpIJPr3pLeW9kjEUdHQxvbqJl7fVMW4gnQuO3kYiYEEHnlnO/c9v5HGthAA4wvSOW9iIV+aP+mY\n/YIq4ieRiGPcVxbw2Ysn8NmLJ/bLM5Ww9YOSikbueWpN15gxgB/dMJOrZ4zg/55ZT3FuKh87Y0zX\nue3VTTy/thyAX7y+hfL6NmYUZXPq6FyeWr6L6ibvB+eFkwtpbg/xzpae26qeOT6fjeUNTC/K4f2z\nRnJSQQbbq5u4aMpQkoIDMw5o7a56fvbqZj5+xmgmDs0kOzWxq0suNz2JjD78MHXO+6GRnXpskqoT\nkXOOxrYQaUnBQ45re2zRTh59dzvr9zQwbkg6n714Aut2N5AUTCAvPYkLJhWyq66FivpWLpk6rN8G\n4LZ2hPnZK5u5/6VNPbpdOxOj3gQSjKumD2fSsCxqm9vJS0/ioilDOanQa9ldtqOWrVVNPPzWVjaW\nNwIwJCOJ+tYQ7aF9v/jkpScRTPASm911rQQSvO5JgJNHZAHw1SumkJoYYGN5A3PH5jM6P62rdSsl\nMUAk4li6Yy8/fWUz63bXs7tuX0v1pKGZzB6TS0c4wmmjc7l+drHvu4FD4Qhbq5p4dWMlr2yo5I2S\nKiYOzeBX/3I6ZbUt/ObNrYwvyCArNZFTR+UyaZj3f4DIYNQeijDxv5/hC5dO5NMXTuiXZyph60eN\nbSE+8MCbbKpoJDmYwORhmaworQPgD7fOZXddK799exsro2Wd0pICNLeHSU8KUJyXxqi8ND5/6UQm\nD8vCOcerGyupbw2xraqJD80uIjUxQE5aUr++Nzm+tXaECSaY7wb9VzW2sbqsjqLcNFaV1ZIUCPDq\nxgrGDskgKzXI5GGZDM9O5Y2SKh58dTPlda20hyN0hPf9X5Zg8OHTR7Fsx96u8YOpiQHGFaRz7sQC\ndtQ0MywrBefgkqlDmTs2r2sYQkNrBz94fiPhiOPiKUOZPSa3a6LI4WoLhVm0dS85aYlMHZ416MeD\nvby+gk//YSlN7eFezycYvG/acG4/bzynjMzu5+hEjk5rR5jJdz/Ll+ZP4lPnn9Qvz1TCNgA27Gng\nsh++BsAV04fzzuZqzOjqcspKCfLLfzmdoVnJLN62l6tnjqA9OvtLRI5MR3RowN6mdlo7IozMTWV7\ndRN3PbGKJdv3MmZIOv9+3nimF2UzOj99wFqijyebyht4bm05wQRj8vAsRuelEXGOdbsbWL5zL396\nbycNbSEykoPkpCVyzcwR3HnRxEN+9lWNbVTUt5GWFCAtKUBhVt/H4IocC01tIU7++kLuunwynzxv\nfL88UwnbAHl3SzW76lr4wKwilmyv4f6XSmhpD/OtD0wjMyXYYxCyiMSP39esO17Vt3bw2ze3Ud7Q\nyu7aVl5cX8G4gnS+cOkk3jet94W6l+3Yy22PLOkxCzoScXzmopOYf/Iw37UEy/GnvrWD6fc8x39f\nMYVbzxnXL888nIRNzTvH0Nxx+V3fnzY6j4dvmTOA0Yic2JSsDYyslEQ+c9G+8T/Prt7D957bwKd+\nv5QbTi/m8mnDmRudWV/b3M6WqiZu/e1i0pO99RH/sWIXU4dnUZCZTHl9K5/+wzLA67n48Q2zfLuo\nqQx+keix8NOJAAAgAElEQVSYVb+uw6aETURE4mb+KcM4f1IB//331TyxrIw/LdrJldOHMzInlV+8\nvoWI8yaV/Pq2eUwYmskPPzyzKykLRxx/X1bGmyVVPLGsjPqWDuaNy2dGUQ6nj83dbxcQkaPVOcnI\nr78UKGETEZG4SkkM8L0PzeBL8yfx9SfX8PRKb8mWa08t4tyJQ5g3Lr9ryEjsD8tAgnHtaUV88NSR\n5Gck8as3tvL6Jm9m/tCsZC6ZOpQ7Lpyg8W5yTChhExERAQozU/jK+6bwzOo9AHz72ml92oPXzPjq\nFVP54mWTaWwL8cK6cv65cjePLS7lscWlXBGdmTquIP242hNZ+lfYKWETEREBoDjPW5x4ZnHOYSdX\nScEE8oJJXD+7mOtnF7NhTwNfe3J118LjmSlBrj3Va5GbXpQTp3cgx6uuFjaNYRMREYFXv3gBx+JH\n4qRhmfz5k2ewp66V1zZVsmDVbh5+axsPv7UNgDf+6wKKcg+83ZhIrEh0HW2/rpfYp19vzGy8mSVH\nvz/fzO4wM/36IiIihy2QYMf0h+Kw7BSun13Mw7fMYdFXL6azgeT9D7xJSUUjkYijvL6VzZWNx+yZ\ncvzZ1yU6wIEcQF9b2P4KzDazk4CHgCeBPwDvi1dgIiIih6sgM5kVX7+U2x9Zwlubq7n4B6/ud/7K\n6cP56hVT+rz3s5w4wsfJsh4R51zIzD4A3O+cu9/MlsUzMBERkSORlZLIQx+fzZ66Vv62rJTXNlaR\nmRKkoTXE0yt389zacu656mSumDac7DTtfSqezoQtmODPJra+JmwdZnYj8C/AVdEy/S0XERFfykgO\nclJhBl+8bDJfvGxf+cbyBq75yZt85W+r+MlLm3j5i+drPTcBYpf1GOBADqCvYd0CnAF8yzm31czG\nAo/ELywREZFjb+LQTB69dQ4fnDWSXXWtvLaxaqBDEp+IOH93ifYpYXPOrXXO3eGc+6OZ5QKZzrnv\nxDk2ERGRY+600Xl857rp5KQl8vTKXQMdzhFzzlHX3DHQYRw3/L5wbl9nib5iZllmlgcsBX5hZj+I\nb2giIiLxkRhI4PJThvHk8l3852MruvaR7Pw6GNzw0Duc+j/PU9fSwfo99fzu7W10hCP88IWN1Lcq\nkTtcnbNE/bqsR1/HsGU75+rN7Fbgd865r5vZyngGJiIiEk83nzmWl9ZX8NelpQDkpiXylyWl/Oym\nUzlz/JABju7g6po7eHdrDQDLd9Zy+yNLaOkI0x6K8MMXNtHQGuLuK6cOcJSDi98Xzu3rGLagmQ0H\nrgeejmM8IiIi/WLSsEzeuesirp9dxF+XlvLLN7ZS19LBf/99Nc75u6WtsrG16/t/+fV7tHSEAfju\nwg0ANLeHBiSuwWzfLFF/Jmx9bWH7BrAQeNM5t8jMxgGb4heWiIhI/JkZ37l2OtfMHElmSpBN5Y38\n519W8GZJNWdP8G8rW2VDOwA5aYlkpyaSl57Esh21tIe85fr9OnDezzq7wwd1l6hz7i/AX2KOtwDX\nxisoERGR/mJmnHWSl5xNHJrJPU+t4akVZb5O2Koa2wB47JNnMHFoJgCLttXwoQffBqC6sX3AYhus\n/L75e18nHRSZ2d/MrCL6+quZFcU7OBERkf6UkhjgwimFPLa4lLueWOXLSQjPrNrNY4t3AjAkI7mr\n/PQxeV3f765v7XGdHJzfdzro6xi23wBPASOir39Ey0RERI4rX7h0EtfMHMEf39vB7Y8uYeGaPawu\nqxvosLr8+++X8vqmKgIJRk7q/mvYP/e5c5k7No9N5Q08u3rPAEU4OPl9DFtfE7YC59xvnHOh6Oth\noCCOcYmIiAyI4rw0vvehGQA8t7acTz6yhCvvf4NVpf5J2gASA9ZjvNXEoZl859rpNLeHuf3RJVQ2\ntA1QdINP51IomSl9Hd7fv/qasFWb2U1mFoi+bgKq4xmYiIjIQEkMJLDgjnP2K/uPPyz11UK1j/7r\n3F7LxwxJ5ze3nA54W3FJ3+xt8v5sc9OSBjiS3vU1YfsE3pIee4DdwHXAzXGKSUREZMBNHZFFRrLX\n2vKHW+eyo6aZh9/aFvclP5Zsr6Gmqb3X57SFvOU7vnDpRGbHjFnr7uQRWYAStsNR29yOGWSl+nOr\n9L7OEt0OXB1bZmafBX4Yj6BERET84Jk7z2FXbQtzxuYxJj+N+17YSHZqkJvPGhuX55VUNHLtz7yZ\nnh+ZO4r//cC0/c7XtXitQNmHaAUqyEgmOzWRjeWNcYnzeFTT3E5OauLgniV6AJ8/ZlGIiIj4UHFe\nGnPH5WNmfPc6b1zbT14uoaK+lca2EGW1LcfsWRX1rfzvgnVdx394dwdjvvxPrrz/dZrbQ/zX4yuZ\n/8PXAXpMNujOzDhlZBarymqPWXzHu73NHb7tDoWjS9j8mYKKiIjEwZyxeSy44xwa20J877kNfPQX\n73DWt18iFI4ck/vf8NA7vLS+gosmF3L/jbO6yleX1fP4klL+vHgnNU37Fsw9lFnFuazb3aBdD/qo\ntrm9T5/rQDmahM1/i9OIiIjE0dQRWbxv2nAeW1zKiuis0WU797ViNbeHusaZ9ZVzjn+s2MWWqiYA\nbj5rDJedPIw7L5rQVeeP7+3c75rsPoyzmlmcQzjiWLe7/rDiOVHtbRrELWxm1mBm9b28GvDWYxMR\nETmh3DhnFPnpSV2L1j78pjcRobUjzMXff5U533qRkoq+D/b//bs7+MwflwFw34dncM6EApKCCXzu\nkoksvfsSANbtrmdGUTa3nzce6NtMxtH5aQCU7j123bbHs/rWjj4lwgPloJMOnHOZ/RWIiIjIYHD6\nmDyWRBOpB14u4d6FG7h0xVB21jSzq87bYeDehRv4+cdm9+l+r2+qBOCUkVmcfdL+S5zmpSeRkRyk\nsS3E3HH5fOmySXxodhHFeWmHvO/wnFQAdtVq14O+CIUdwYB/R3sdTZfoIZnZfDPbYGYlZvblA9Q5\n38yWm9kaM3s1pvxz0bLVZvZHM0uJZ6wiIiKH65PnjmPq8Cz+3z/W8v3nN/K+acO4cU4xb2+upiMc\noaSi4aATE15cV87CNeVcPWMET3/mHAoyk3vUuWByIQCzR+eSkGCML8joU2wZyUGyUxPZdQwnRhzP\nQhFHICGuadFRiVtkZhYAHgAuB6YCN5rZ1G51coCfAlc7504GPhQtHwncAcx2zp0CBIAb4hWriIjI\nkQgGErh+dhE1Te3MKs7hB9fPZGZxDvWtISZ89Rku/sFrnPXtl2jt6H1c26/e2ArABZMPvHnQd66d\nxrc+cAoXRhO3wzE8O4XddUrY+iLiHAH/5mt9W4ftCM0BSpxzWwDM7E/ANcDamDofAZ5wzu0AcM5V\ndIst1cw6gDRgVxxjFREROSLXn15MS0eED59eTEpigLlj83vU+dkrm7nlrDGkJgVIDga6ypvaw5wx\nLp/3zxx5wPunJQX56NzRRxRbUW4q26ubj+jaE0044gj4dON3iG+X6EggdlpLabQs1kQg18xeMbMl\nZvZxAOdcGfA9YAfezgp1zrnnenuImd1mZovNbHFlZeUxfxMiIiIHk5YU5N/PH09eujcRYMyQdJ7+\nzNn71fnRi5uY+Y3nuSM6ueCBl0t4Y1MVVQ1tDM9JweKUKMwszmFTRSN7o8uByIFFIq7H3qx+MtCN\nf0HgNOAK4DLgbjObaGa5eK1xY/Fmo6ZH9y/twTn3kHNutnNudkGB9qMXEZGBd8rIbOafPAyAe6+b\n3lW+cE05W6uauHfhBm761buU1bZQkNFz3NqxMm+c19r3+JLSuD3jeBF2/m5hi2eXaBlQHHNcFC2L\nVQpUO+eagCYzew2YET231TlXCWBmTwBnAo/GMV4REZFj5mc3ncre5g7y0pN4ITq5AOCC772yX73e\nJhocKzOLczhtdC7fXbie608v9vWyFQMtHHG+3ZYK4tvCtgiYYGZjzSwJb9LAU93qPAmcbWZBM0sD\n5gLr8LpC55lZmnntxBdFy0VERAYFM+vqJv3B9TM566SeY9ugb4vgHqlgIIH/mj+ZjrDj7c1VcXvO\n8eCETdiccyHg08BCvGTrMefcGjO73cxuj9ZZBzwLrATeA37pnFvtnHsXeBxYCqyKxvlQvGIVERGJ\np/TkIP92zriu4/s+PIMHbzoVgJNHZMf12bNG5ZCdmsh9z2+iqU3bVB1I2Pk7YTPnjp8dpmbPnu0W\nL1480GGIiIj0qqU9TGrSvlmiHeEIif2wlsRL68v5xMOLueeqqdx81ti4P2+wcc4x9q4F3HnRBD53\nycR+e66ZLXHO9WmF5YGedCAiInLCiE3WgH5J1gAunDyU6UXZ/G25VsjqTTjiNV75uYVNCZuIiMgJ\nYN64fNbtqqc9FBnoUHwn7JSwiYiIiA9ML8qmPRxhw56+b0x/olALm4iIiPjCjKIcAFaU1g5wJP7T\nlbD5eB02JWwiIiIngKLcVHLTElmphK2HSLSXWDsdiIiIyIAyM6YX5bB8Z98StpWltXzjH2tpaQ+z\np66Vtbvq4xzhwOkaw+bffC2uOx2IiIiIj1wwqYB7/rGW97bWMGds3kHr/m1ZGb95cxu/fnNrV9n6\nb84nJTFwkKsGJ41hExEREd+4Yc4okgIJvLiu/JB1t1c39yh7d2tNPMIacPsSNv+mRf6NTERERI6p\nlMQAI3NTKa1tOWTdbdVNjC9I55SRWfztU2eSHEzgS4+v4K2S42+Lq33LegxwIAfh49BERETkWCvK\nTaV078ETtnDEsbOmmUumDuPpz5zDrFG5TB6WSXl9Gx/55bv9FGn/iURb2BI0S1RERET8oCg3lbK9\nPbs7Y9U0tdMRdozISekqu/TkYV3fH0/bWoLGsImIiIjPjMxJpaqxndaO8AHrNLd7m8RnJO+bm3j7\neeO55awxAJTXt8U1xv6mnQ5ERETEV4py0wAO2i3a1OYlc2lJ+xK2QIIxP9rKtn7P8bXER0QtbCIi\nIuInRbmpAJQepFu0s4UtPXn/JTwmDs0EYGP58bW9VUg7HYiIiIif9KmFrb1nCxtAbnoShZnJbNjT\nGL8AB0DnGDbtdCAiIiK+UJiZTGLADtEl2nsLG8CkYZms2VUXt/gGQsSphU1ERER8JCHBGJmTyvbq\nph7nQuEIO2ua9yVsST03RDpvYgHr9zQcV0mbZomKiIiI75w8MpsVvewpet8LGznnuy/zjafXApCW\n1LOF7UOnFZMYMJ5cvivucfaXzhY2dYmKiIiIb5w6Kpddda3sqWvdr/yNTd4uBg2tnV2iPVvYstMS\nmTM2j1c2VMQ/0H4SCnsJW1AJm4iIiPjFKSOyANjQbbZn90kGycHe04RzJxSwsbyRyobjYz22znXY\ntNOBiIiI+MbQLG8Hg6puCVcwsH/CYgdIYGaNygVgZWnPbtXBKBLxvmoMm4iIiPhGQWYyAJWN+yds\nNU3tfbr+lJFZBBKs13Fwg5E2fxcRERHfSU8Okp4U6NGlWdHQxnkTCw55fVpSkIlDM1leWsfv3t7G\n/B++hnOOz/95OSd/7dk4RR0/g2Hz956jCUVEROS4V5CZvF/C1toRprqxjRnFOfz4hlm0HGSvUYCZ\nxdksWLWH1zZWArCjppknlpUB3ubwB+pO9aPOnQ6CCf5tx/JvZCIiIhI33RO21WV1RBxMG5lNdloi\nw7JTDnr9jKIc6lo6uo4Xb9vb9X1fu1b9Yt9OBwMcyEH4ODQRERGJl8KsFHbX7dvtYHl0PNrM4pw+\nXX/q6Nz9jl9YV971/a7a1u7Vfa1rpwNNOhARERE/GV+QwY6aZlqjXZ8byxsoyEzumpBwKCcVZHR9\nf8a4/P0TtroDb3vlR2Ft/i4iIiJ+NKEwg4iDrVXeFlXl9W0Myzp4N2ishATjxjmjOPukIdx58QQ6\noovPwsE3lvejzuVJ/LzTgSYdiIiInIAmDPVayDaWNzBleBYVDW2MOMS4te7+74PTAGhuDxFIsK6W\nqm8+vZa65nY+f+mkYxt0HNQ2t/OL17cC2ulAREREfGZ0XjqwrzWssqGVwqy+dYd2l5YUZHxB+n5l\nP36phAdeLjm6IPvBnvp94+38vKyHEjYREZETUGpSgOzURJ5dvYey2haqm9opyDy8FrZYI3JSAZhR\nlN1Vdu/CDb7fvip2P1VNOhARERHfKcxMZlVZHWd9+yWcg6FH2MIGdI1/++CpRWz79hU8/7lzgf1n\nj/rRbiVsIiIi4metof0Xxy08iha2znXbqqPbXY0ryCApkMC26qYjD7AfxCZsfu4S1aQDERGRE1RN\n4/4L3Bb2cUmP3txy5ljW727gpjNGA15rVVFuKjtrmo8qxngrj0nYNOlAREREfOfa04r2Oz7SSQcA\n2WmJPPix0/ZrpRuVn8b2an8nbLGtjH5e1kMJm4iIyAnqa1dO5bnoWDOAIRlHnrD1ZlReGjuqm3HO\nHbryAOncRxQ0hk1ERER8KBhIYELhvh0LEgPHNi0YlZdGQ1uI2uaOQ1ceIKFwpOt7dYmKiIiIL1kc\nB9qPyksDYIePx7GFI45JQzN57ysXkZIYGOhwDiiuCZuZzTezDWZWYmZfPkCd881suZmtMbNXY8pz\nzOxxM1tvZuvM7Ix4xioiInKiuvvKqdx95dRjft/R+d5iutt9nLCFIo6UxAQKD2NbroEQt1miZhYA\nHgAuAUqBRWb2lHNubUydHOCnwHzn3A4zK4y5xY+AZ51z15lZEpAWr1hFREROZP969ti43Lc4z1tM\n188zRcMR5+uxa53i2cI2Byhxzm1xzrUDfwKu6VbnI8ATzrkdAM65CgAzywbOBX4VLW93ztXGMVYR\nERE5xtKSghRkJrPdx2uxdYQjBBP8P0IsnhGOBHbGHJdGy2JNBHLN7BUzW2JmH4+WjwUqgd+Y2TIz\n+6WZpdMLM7vNzBab2eLKyspj/R5ERETkKIzKS/P9GLYTvYWtL4LAacAVwGXA3WY2MVp+KvAz59ws\noAnodQycc+4h59xs59zsgoKCfgpbRERE+mJ0dGkPvwpFHMHAiZ2wlQHFMcdF0bJYpcBC51yTc64K\neA2YES0vdc69G633OF4CJyIiIoNIcV4au+tbaeu2DZZfhMLO18t5dIpnwrYImGBmY6OTBm4AnupW\n50ngbDMLmlkaMBdY55zbA+w0s0nRehcBaxEREZFBZeyQdJyDrVX+HMcWijgCg2AMW9xmiTrnQmb2\naWAhEAB+7ZxbY2a3R88/6JxbZ2bPAiuBCPBL59zq6C0+A/w+muxtAW6JV6wiIiISH6eMzAZgZWkd\nk4dlDXA0PYUjERIHQZdoXDd/d84tABZ0K3uw2/G9wL29XLscmB3P+ERERCS+xg1JJzM5yMrSWq6f\nXXzoC/pZSJMORERE5ESXkGBMK8pmZWndQIfSK41hExEREQGmF+Wwbne9LycehAfJGDb/RygiIiKD\n2szibDrCjtVl/mtlCw2SMWxK2ERERCSu5o3LJyM5yM9f3TLQofQQCmsMm4iIiAg5aUm8f9YI3t5c\nPdCh9BCKaAybiIiICABFuWk0tIVoaO0Y6FD2E444ggH/p0P+j1BEREQGveHZKQDsqWsd4Ej2F4pE\n1MImIiIiAjA8OxWA3X5L2DSGTURERMTT2cK2u65lgCPZxzmnMWwiIiIinQqzkgGoqG8b4Ej2iTjv\nq8awiYiIiADJwQApiQnU+2jSQSgSAVCXqIiIiEin7NRE6ltCAx1Gl1DYa2JTl6iIiIhIVFZKos9a\n2LyETS1sIiIiIlFZqf5K2MLRhC1RY9hEREREPFkpwWPSJbq1qom7nljF3qb2o7pPKDx4xrAFBzoA\nEREROTFkpSayparpqO/z+ceWs2xHLY1tIe6/cdYR36ezS1Rj2ERERESislISqW85+i7RkopGAP6x\nYhfPrNp9xPfp7BLVsh4iIiIiUVmpQepbQzjnjuo+bR0R/u2csZwyMotvPL32iO+nFjYRERGRbrJS\nEglHHE3t4SO+RzjiaA9HSE8O8uHTR7G7rpXSvUe2e8JgGsOmhE1ERET6RVZqIsBRdYu2h7wkKzkY\nYFZxDgDLdtYe0b3UwiYiIiLSTVZKNGE7iqU9Wju81rmUxAQmDcskKZjA6rK6I7qXxrCJiIiIdJPd\n1cJ25Et7tIY6E7YAiYEExuans6Wy8Yju1RHtElULm4iIiEhUVqq3mtjRdIm2dXhJVkqil8KML0xn\nc+WRLRVSXt8KQEFm8hHH01+UsImIiEi/OCZdotEWtuRgAIDxBRnsqGnuGtsWa8yX/8n/Llh3wHtt\nq24GYFR+2hHH01+UsImIiEi/6Jx08PnHVlB3hK1srd1a2KYOzyIccaws3X/iQWd350OvbTngvbZX\nN5GfntSVSPqZEjYRERHpF5kp+zZYWrh6zxHdo2vSQbSF7czxQ0gweGVDZa/1DmZbVTOjB0HrGihh\nExERkX6y3ybrRzjOv61zWY9EL2HLTktkQmEm6/fU71evsyXuYDZVNDC+IOPIAulnSthERESk31U2\ntB3RdZ0tZ8nBfSlMdloiDa2hXusdSEVDK1WN7UwZnnVEcfQ3JWwiIiLSb3720VOBfTM0D9e+ddgC\nXWWZycEeCVvLIRK2dbsbAJSwiYiIiHR3+bThTCjMoKL+yFrYOrtEOycdgDc2rrHtwC1szjnKalu6\nFsoFKN3rzRAdOyT9iOLob0rYREREpF8VZiVT3nBkLWxtHfsv6wGQkRKkodtSIS0x+5Xe+aflnPXt\nl3h8yc6ussZoi1xGzEQIP1PCJiIiIv0qLz2ZvU3tfOLhRTy5vOywru2+rAdAZkoijW0hnNvXghbb\nJfrUil0AbCzftyNCU7RFLi2ma9XPBkdaKSIiIseNnNREtlU3s626mZfWV3DNzJF9vrYt1HMMW0Zy\nkI6woy0U6SrvPks0PSnArtqWruPGtjAZyUESBsG2VKAWNhEREelnOWn7FqpNSzq8Fq6qxnYykoP7\nLRGSFe3WjJ140H2W6Kmjc/dL2JraQqQnD47WNVDCJiIiIv2scxN44LB3GdhW3dRjsdvM6D1ix7F1\nnyU6MieVstp94+Ya20OkJw+ejsbBE6mIiIgcF3LSkrq+r2xsIxxxBPrYNbm9upmp3ZbiyIgmXo1t\nISIRR2N7iPue3wjAX24/g2FZKTyxtIyqxjbaQxGSggk0tYW6rhsM1MImIiIi/SonpoUtHHFUHGDG\naGNbiE88vIhtVU0AhMIRdtb03E6qc8ur+pYQ1z34FtPveY6K6MK800ZmU5yXRl6GlyTWtrQD0S7R\nJCVsIiIiIr2KHcMGUNvc+0bwS7bv5aX1FXz2z8sB+N3b2wlFHKeOyt2vXm66l4ztbW5n6Y79N4Hv\n3BEhN/rM2uYO3tlSzaJtewdVl6gSNhEREelX++0pyr4lNrprjpav3e3tE/rY4p3MHp3LRVMK96uX\nm7YvYbNuPasWLchJjdZpaueGh94BoD186P1G/SKuCZuZzTezDWZWYmZfPkCd881suZmtMbNXu50L\nmNkyM3s6nnGKiIhI/5k0LJM5Y/P4+lVTAWg4QMJW2eh1a7aHIuypa2X9ngYunFLYlYR16myxK6tt\nwTn40vxJ/PjGWdw0b1SPOntjWvM27mk4dm8qzuLWFmhmAeAB4BKgFFhkZk8559bG1MkBfgrMd87t\nMLPCbre5E1gHDI6NvkREROSQUhIDPPbJM9hY7iVMB2phq4rZIP75tXsAOH1MXo96iYEE0pMC/PzV\nLYA3a/TqGSO4esaIrjqdCduynXu7yqaOGDzpRTxb2OYAJc65Lc65duBPwDXd6nwEeMI5twPAOVfR\necLMioArgF/GMUYREREZIJ1jyA6UsHW2sAG8s6UGgGFZKb3WbYrZiiqjl/XVOrtNf/7qFpICCfzh\n3+byoxtmHlngAyCeCdtIYGfMcWm0LNZEINfMXjGzJWb28ZhzPwS+BBy0g9nMbjOzxWa2uLKy8ljE\nLSIiIv2gc1mN2AVvY1U2tJMfnVDw7tZqwNuH9FDSepn9mZYUIDHgdaXOHpPLmeOHdK3fNhgM9KSD\nIHAaXkvaZcDdZjbRzK4EKpxzSw51A+fcQ8652c652QUFBXEOV0RERI6V9OguB01t4R7nnHNsLG9g\n1qgcctMSqWpsJzctcb9N3w+kt/XVzKyrla0w89BJn9/EM2ErA4pjjouiZbFKgYXOuSbnXBXwGjAD\nOAu42sy24XWlXmhmj8YxVhEREelnwUACqYkBapraepz76Sub2VHTzMVThjKjOAeAwszeu0MBvnvt\n9K7vD7Rcx/Bs7/ohGUrYYi0CJpjZWDNLAm4AnupW50ngbDMLmlkaMBdY55y7yzlX5JwbE73uJefc\nTXGMVURERAZAS0eY3769nXsXrv//7d17rBxlGcfx7++UQktvBKhYi1BRQAlokVMUkVoSQMVLEQkX\nCRaKFxDxbtRooiZWjFX+ADSA9QIGVKwGuUm9QAUFlFJKKRRMRYxVtBQBOVA4tH38Y95jd89ud8+W\nmZ3Z098nmWRmzjszzzydzD5958aTG7Y8wfntm9cwpk+89aBpHPzS7L1rrb79eeKsLX1Eze5hgy1f\nWNi9B3vYCntKNCI2SvowsAQYA3wvIu6TdFb6+8URsVrSjcBKsnvVFkXEqqJiMjMzs2r61s1/4cF/\nDbBoXj+DGzfz9OAmPnn0fkwZP5bTDtubNY8OMHvf3Ue0rmb3sMGWl+gO3RfXSwp9xW9E3ADcMGze\nxcOmFwILW6xjKbC0gPDMzMysQn6z+t/88aHH2GfqRGDLqzh2nbAjF55y8IjXs7VLojuNzXre+oa/\nXbcHlP3QgZmZmW3HFrzrwLrpky69g1kLfgPAlJ076wk7bmb23rWhhxmGO/bAFwO99f61IYqIsmPI\nTX9/fyxbtqzsMMzMzKwDzz6/iZseWMeHrlheN//y+Ycye7+RvwFicONmntgw2PLhhIHnNjZ9irQM\nku6KiP6RtHUPm5mZmZVq3NgxHHvQNH7/mSPr5g//SHw7O+7Q17JYg+av/OgFLtjMzMysEqZNGV83\nPZMZnFQAAAoHSURBVGV877zYtmgu2MzMzKwSxvQN+6j7+N57mrMoLtjMzMysMmpfajtpXG9eviyC\nM2FmZmaVcfOn3sT6gUGeeGaQvr7ee/1GUVywmZmZWWVMGjc2fZR9QtmhVIoviZqZmZlVnAs2MzMz\ns4pzwWZmZmZWcS7YzMzMzCrOBZuZmZlZxblgMzMzM6s4F2xmZmZmFeeCzczMzKziXLCZmZmZVZwL\nNjMzM7OKU0SUHUNuJD0K/K1m1u7A+pLCqSrnpJFzUs/5aOSc1HM+GjknjZyTes3ysXdETB3JwqOq\nYBtO0rKI6C87jipxTho5J/Wcj0bOST3no5Fz0sg5qfdC8+FLomZmZmYV54LNzMzMrOJGe8F2adkB\nVJBz0sg5qed8NHJO6jkfjZyTRs5JvReUj1F9D5uZmZnZaDDae9jMzMzMep4LNjMzM7OKc8FmZmZm\nVnEu2MzMzMwqbrst2CTtJelqSd+T9Nmy46kCSUdIuljSIkm3lR1P2ST1SVog6UJJ88qOpwokzZF0\nazpO5pQdT1VImiBpmaS3lx1L2SS9Kh0fiyWdXXY8VSDpOEnfkfQTSceUHU/ZJO0j6buSFpcdS5nS\neeOydGyc2q59TxZsqchaJ2nVsPlvkfSgpDUjKMIOAhZHxHzg4MKC7ZI8chIRt0bEWcB1wGVFxlu0\nnI6RucCewPPA2qJi7ZacchLAADAO56TWZ4Criomye3I6j6xO55ETgcOLjLcbcsrJ1RHxfuAs4KQi\n4y1aTvl4KCLOLDbScnSYn+PJ6pD3A+9su+5efK2HpNlkPxqXR8SBad4Y4M/A0WQ/JHcCpwBjgPOG\nrWI+sAlYTPYD9MOI+H53oi9GHjmJiHVpuauAMyPiqS6Fn7ucjpH5wOMRcYmkxRFxQrfiL0JOOVkf\nEZsl7QGcHxFt/1dYZTnl5DXAbmRF7PqIuK470ecvr/OIpHcCZ5OdW6/sVvxFyPnc+k3giohY3qXw\nc5dzPnr+vDpch/mZC/wyIlZIujIi3tNq3TsUGnlBIuIWSTOGzT4UWBMRDwFI+jEwNyLOAxouU0j6\nFPDFtK7FQE8XbHnkJLXZC3iyl4s1yO0YWQsMpsnNxUXbHXkdI8njwE5FxNlNOR0nc4AJwAHABkk3\nRERPHi95HSMRcQ1wjaTrgZ4u2HI6RgR8jezHuWeLNcj9PDLqdJIfsuJtT2AFI7ji2ZMF21ZMB/5e\nM70WeF2L9jcCX5L0HuDhAuMqU6c5ATiTHi9eW+g0Hz8HLpR0BPC7IgMrUUc5kXQ88GZgF+CiYkMr\nTUc5iYjPA0g6ndQDWWh03dfpMTKH7FLPTsANhUZWnk7PJecCRwFTJL0iIi4uMrgSdHqM7AYsAA6W\n9LlU2I1mW8vPBcBFkt4GXNtuJaOpYOtIRKwCRlVXbB4i4otlx1AVEfEMWQFrSUT8nKyQtWEi4gdl\nx1AFEbEUWFpyGJUSEReQ/TgbEBGPkd3Pt12LiKeBM0bavicfOtiKfwAvrZneM83bnjkn9ZyPRs5J\nI+eknvPRyDmp53y0lkt+RlPBdiewr6SXSdoROBm4puSYyuac1HM+GjknjZyTes5HI+eknvPRWi75\n6cmCTdKPgNuB/SWtlXRmRGwEPgwsAVYDV0XEfWXG2U3OST3no5Fz0sg5qed8NHJO6jkfrRWZn558\nrYeZmZnZ9qQne9jMzMzMticu2MzMzMwqzgWbmZmZWcW5YDMzMzOrOBdsZmZmZhXngs3MzMys4lyw\nmVlbkga6vL1Fkg7IaV2bJK2QtErStZJ2adN+F0kf2obtSNJNkian6dxzJunzku6TtDLtU7tvAzdb\nxwxJq9q0mSrpxm2P1Mzy5oLNzLpOUsvvGEfE+yLi/pw2tyEiZkbEgcB/gHPatN8F6LhgA44F7omI\n/27Dsm1JOgx4O/DaiHg12cfE/956qW0TEY8Cj0g6vIj1m1nnXLCZ2TZJvTA/k3RnGg5P8w+VdLuk\nuyXdJmn/NP90SddIugn4raQ5kpZKWizpAUlXSFJqu1RSfxofkLRA0j2S7pC0R5r/8jR9r6SvjLBH\n63Zgelp+oqTfSlqe1jE3tfka8PLUg7Uwtf102seVkr68lXWfCvyiTc5mpF64lWnbe3WwL9OA9RHx\nHEBErI+If6blZ6Vc3yPpT5ImpW3dmvZvuaQ3NIlnjKSFNfv2wZo/X532ycyqICI8ePDgoeUADDSZ\ndyXwxjS+F7A6jU8GdkjjRwE/S+OnA2uBXdP0HOBJsg8h95EVU0PrWwr0p/EA3pHGvw58IY1fB5yS\nxs9qFmNt7MAY4KfAW9L0DsDkNL47sAYQMANYVbP8McCl6W99abuzm2znb8CkNjm7FpiXxucDV490\nX4CJwArgz8C3gTel+TsCDwGzavMP7AyMS/P2BZal8f/vH/CBmnzuBCwDXpampwP3ln3sefDgIRta\nXpYwM2vhKOCA1CkGMFnSRGAKcJmkfcmKrbE1y/w6Iv5TM/2niFgLIGkFWTHx+2HbGSQraADuAo5O\n44cBx6XxK4FvbCXO8Wnd08m+4/frNF/AVyXNBjanv+/RZPlj0nB3mp5IVgDdMqzdrhHx1FZiGHIY\ncHwa/yFZATqifYmIAUmHAEcARwI/kfRZspw8EhF3pnb/BZA0AbhI0kxgE7DfVvbt1ZJOSNNT0r79\nFVgHvKTN/phZl7hgM7Nt1Qe8PiKerZ0p6SLg5oh4l6QZZL1lQ54eto7nasY30fyc9HxERJs2rWyI\niJmSdib7+PI5wAVkl/umAodExPOSHgbGNVlewHkRcUmb7WyU1BcRmzuMb8QiYhNZPpdKuheYR1aw\nNfNx4N/Aa8j+rZ5t0kbAuRGxpMnfxgEbXmjMZpYP38NmZtvqV8C5QxOpJweyXpp/pPHTC9z+HcC7\n0/jJ7RpHxDPAR4BPpocepgDrUrF2JLB3avoUMKlm0SXA/NR7iKTpkl7UZBMPAvu0CeO2mlhPBW4d\n6b5I2j/1Wg6ZSXYZ9kFgmqRZqd2kmv17JBWQp5FdEh5uCXC2pLFp2f1SzxxkPXItnyY1s+5xwWZm\nI7GzpLU1wyfIip/+dLP6/WT3XkF2me88SXdTbC/+x4BPSFoJvILsfriWIuJuYCVwCnAFWfz3Au8F\nHkhtHgP+oOw1IAsj4ldklylvT20XU1/QDbme7L68Ic1ydi5wRor5NOCjHezLRLJLzfendgcAX4qI\nQeAk4EJJ95Bd8h1Hdp/bvDTvlTT2bgIsAu4Hlit71cclbPk3OzLtk5lVgLZcaTAz6x3pEueGiAhJ\nJ5PdtD+33XIFxjMNuDwijm7buHHZSu1LiukWYG5EPF5mHGaW8T1sZtarDiG7qV7AE2RPXZYmIh6R\n9B1Jk6Pzd7FVal8kTQXOd7FmVh3uYTMzMzOrON/DZmZmZlZxLtjMzMzMKs4Fm5mZmVnFuWAzMzMz\nqzgXbGZmZmYV9z8FTfl7DgsrZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xeb10a4be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "find_lr.plot_loss(skip_begin=35, skip_end=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot and data show that effective learning (steady drop in loss) begins around $10^{-2.5}$, and\n",
    "learning becomes erratic and the loss quickly explodes above $10^{-1}$. So the learning rates will cycle between these two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from cyclic.rate_cycler import CyclicLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the min/max learning rates determined above, do test runs with the sample_train and sample_test data to determine the best choices for number of embedding, attention and GRU dimensions. Start with the smallest and see if increasing these dimensions results in significant performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clr = CyclicLR(epochs=4, num_samples=8000, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "model = build_model()\n",
    "opt = SGD(momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 299s 37ms/step - loss: 0.6542 - acc: 0.6070 - val_loss: 0.5866 - val_acc: 0.6930\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 322s 40ms/step - loss: 0.5619 - acc: 0.7103 - val_loss: 0.5367 - val_acc: 0.7290\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 334s 42ms/step - loss: 0.5232 - acc: 0.7278 - val_loss: 0.5176 - val_acc: 0.7310\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 327s 41ms/step - loss: 0.5102 - acc: 0.7364 - val_loss: 0.5102 - val_acc: 0.7410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xeaf610eb8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sample_train_matrix, sample_y_train, validation_data=(sample_test_matrix, sample_y_test),\n",
    "          batch_size=batch_size, epochs=4, callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAFPCAYAAAASmYlZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xeb14dedd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clr.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "model = build_model()\n",
    "opt = SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 305s 38ms/step - loss: 0.6838 - acc: 0.5785 - val_loss: 0.6686 - val_acc: 0.6485\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 308s 39ms/step - loss: 0.6568 - acc: 0.6485 - val_loss: 0.6452 - val_acc: 0.6470\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 319s 40ms/step - loss: 0.6308 - acc: 0.6634 - val_loss: 0.6200 - val_acc: 0.6695\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 301s 38ms/step - loss: 0.6069 - acc: 0.6791 - val_loss: 0.6077 - val_acc: 0.6610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xeb14bd978>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sample_train_matrix, sample_y_train, validation_data=(sample_test_matrix, sample_y_test),\n",
    "          batch_size=batch_size, epochs=4, callbacks=[clr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nesterov momentum did not improve performance, but the algorithms should also be tried without the cyclic learning rate, after which the Adam optimizer will be tried. A first experiment with Adam showed excessive overfitting, so it will be tried with a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_trial(attention_dim=attention_dim, GRU_dim=GRU_dim, embedding_matrix=embedding_matrix, embedding_dim=embedding_dim,\n",
    "              momentum=0.9, batch_size=batch_size, epochs=4, cyclic=False, nesterov=False, adam=False, drop_pct=0.5,\n",
    "              X=sample_train_matrix, y=sample_y_train, X_val=sample_test_matrix, y_val=sample_y_test,\n",
    "              word_index=word_index, scale_factor=0.5, base_lr=10**(-2.5), max_lr=1e-1):\n",
    "    K.clear_session()\n",
    "    try:\n",
    "        del model\n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if adam is True:\n",
    "        opt = Adam()\n",
    "        model = build_model(attention_dim=attention_dim, GRU_dim=GRU_dim, embedding_matrix=embedding_matrix,\n",
    "                            embedding_dim=embedding_dim, word_index=word_index, drop=True, drop_pct=drop_pct)\n",
    "    else:\n",
    "        opt = SGD(momentum=momentum, nesterov=nesterov)\n",
    "        model = build_model(attention_dim=attention_dim, GRU_dim=GRU_dim, embedding_matrix=embedding_matrix,\n",
    "                            embedding_dim=embedding_dim, word_index=word_index)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "    \n",
    "    if cyclic is True:\n",
    "        clr = CyclicLR(epochs=epochs, num_samples=8000, batch_size=batch_size, scale_factor=scale_factor,\n",
    "                       base_lr=base_lr, max_lr=max_lr)\n",
    "        model.fit(X, y, validation_data=(sample_test_matrix, sample_y_test),\n",
    "                  batch_size=batch_size, epochs=epochs, callbacks=[clr])\n",
    "    else:\n",
    "        model.fit(X, y, validation_data=(sample_test_matrix, sample_y_test),\n",
    "                  batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 306s 38ms/step - loss: 0.6676 - acc: 0.6054 - val_loss: 0.6408 - val_acc: 0.6365\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 299s 37ms/step - loss: 0.6214 - acc: 0.6491 - val_loss: 0.6315 - val_acc: 0.6180\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 300s 37ms/step - loss: 0.5806 - acc: 0.6944 - val_loss: 0.5781 - val_acc: 0.6850\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 300s 37ms/step - loss: 0.5461 - acc: 0.7208 - val_loss: 0.5312 - val_acc: 0.7360\n"
     ]
    }
   ],
   "source": [
    "run_trial()  # SGD without cyclic learning rate or Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 314s 39ms/step - loss: 0.6696 - acc: 0.5975 - val_loss: 0.6410 - val_acc: 0.6380\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 301s 38ms/step - loss: 0.6056 - acc: 0.6769 - val_loss: 0.5780 - val_acc: 0.6955\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 301s 38ms/step - loss: 0.5555 - acc: 0.7107 - val_loss: 0.5543 - val_acc: 0.6895\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 300s 38ms/step - loss: 0.5304 - acc: 0.7223 - val_loss: 0.5207 - val_acc: 0.7180\n"
     ]
    }
   ],
   "source": [
    "run_trial(nesterov=True)  # SGD without cyclic learning rate but with Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 306s 38ms/step - loss: 0.5970 - acc: 0.6629 - val_loss: 0.5262 - val_acc: 0.7240\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 300s 38ms/step - loss: 0.4960 - acc: 0.7429 - val_loss: 0.4793 - val_acc: 0.7605\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 302s 38ms/step - loss: 0.4610 - acc: 0.7735 - val_loss: 0.4823 - val_acc: 0.7690\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 300s 38ms/step - loss: 0.4293 - acc: 0.7981 - val_loss: 0.4779 - val_acc: 0.7580\n"
     ]
    }
   ],
   "source": [
    "run_trial(adam=True)  # Adam optimizer with default 50% dropout in feed to prediction layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 50% dropout somewhat lessened the gap between the training and validation accuracy scores, which was large without dropout, but the model is still overfitting. And the validation accuracy decreased during the fourth epoch to below the second epoch score, while the training accuracy continually increased. Reducing the GRU and/or attention dimensions could help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 161s 20ms/step - loss: 0.6259 - acc: 0.6289 - val_loss: 0.5571 - val_acc: 0.7190\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 161s 20ms/step - loss: 0.5257 - acc: 0.7352 - val_loss: 0.5607 - val_acc: 0.6950\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 158s 20ms/step - loss: 0.4822 - acc: 0.7635 - val_loss: 0.4759 - val_acc: 0.7655\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 163s 20ms/step - loss: 0.4570 - acc: 0.7795 - val_loss: 0.4626 - val_acc: 0.7740\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=64, attention_dim=32, adam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 101s 13ms/step - loss: 0.6384 - acc: 0.6294 - val_loss: 0.5801 - val_acc: 0.7245\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 97s 12ms/step - loss: 0.5394 - acc: 0.7241 - val_loss: 0.4993 - val_acc: 0.7570\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 97s 12ms/step - loss: 0.4960 - acc: 0.7493 - val_loss: 0.5792 - val_acc: 0.6790\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 99s 12ms/step - loss: 0.4826 - acc: 0.7571 - val_loss: 0.4687 - val_acc: 0.7740\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=32, attention_dim=16, adam=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller attention networks produce quicker models with little or no overfitting through four epochs, but some erratic behavior on the validation set. These might produce good results on the full dataset running for only a few epochs. The validation loss and accuracy for 32/16 network were betterthan the corresponding training scores, indicating that training for more epochs would improve the results. Try some other combinations for the cyclic learning rate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 304s 38ms/step - loss: 0.6508 - acc: 0.6105 - val_loss: 0.6387 - val_acc: 0.6305\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 297s 37ms/step - loss: 0.5764 - acc: 0.6994 - val_loss: 0.5551 - val_acc: 0.7165\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 307s 38ms/step - loss: 0.5421 - acc: 0.7275 - val_loss: 0.5475 - val_acc: 0.7065\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 305s 38ms/step - loss: 0.5287 - acc: 0.7310 - val_loss: 0.5287 - val_acc: 0.7310\n"
     ]
    }
   ],
   "source": [
    "run_trial(momentum=0.8, cyclic=True)  # Reduced momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing momentum does not give any performance gain, so keep it at 0.9. Try increasing the size of the word embeddings and the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_200 = store_embeddings(vector_file='glove.6B.200d.txt')\n",
    "embedding_matrix_200 = create_embedding_matrix(embeddings=embeddings_200, embedding_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 327s 41ms/step - loss: 0.6278 - acc: 0.6320 - val_loss: 0.5544 - val_acc: 0.7080\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 323s 40ms/step - loss: 0.5307 - acc: 0.7185 - val_loss: 0.5117 - val_acc: 0.7465\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 328s 41ms/step - loss: 0.5184 - acc: 0.7276 - val_loss: 0.5037 - val_acc: 0.7510\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 319s 40ms/step - loss: 0.4980 - acc: 0.7485 - val_loss: 0.4960 - val_acc: 0.7460\n"
     ]
    }
   ],
   "source": [
    "# Use the default GRU/attention dimensions of 128/64.\n",
    "run_trial(embedding_matrix=embedding_matrix_200, embedding_dim=200, cyclic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 504s 63ms/step - loss: 0.6466 - acc: 0.6209 - val_loss: 0.5764 - val_acc: 0.7185\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 494s 62ms/step - loss: 0.5289 - acc: 0.7266 - val_loss: 0.5095 - val_acc: 0.7350\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 494s 62ms/step - loss: 0.5060 - acc: 0.7404 - val_loss: 0.5528 - val_acc: 0.6930\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 508s 64ms/step - loss: 0.4864 - acc: 0.7549 - val_loss: 0.4884 - val_acc: 0.7605\n"
     ]
    }
   ],
   "source": [
    "# Increase the dimensions of the attention network\n",
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200, embedding_dim=200, cyclic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 328s 41ms/step - loss: 0.6891 - acc: 0.5726 - val_loss: 0.6605 - val_acc: 0.6110\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 339s 42ms/step - loss: 0.5834 - acc: 0.6923 - val_loss: 0.5355 - val_acc: 0.7270\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 331s 41ms/step - loss: 0.5237 - acc: 0.7306 - val_loss: 0.5254 - val_acc: 0.7195\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 336s 42ms/step - loss: 0.5102 - acc: 0.7351 - val_loss: 0.5069 - val_acc: 0.7420\n"
     ]
    }
   ],
   "source": [
    "# Decrease the attention dimension relative to the GRU dimension.\n",
    "run_trial(attention_dim=32, embedding_matrix=embedding_matrix_200, embedding_dim=200, cyclic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 128/32 attention network is very comparable to the 128/64, althought the more complex 200/100 scores better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Increase the vocabulary to 80,000\n",
    "word_index_80k = {}\n",
    "for ix, (word, _) in enumerate(words.most_common(80000)):\n",
    "    word_index_80k[word] = ix + 1 # The zero index is reserved for masking out-of-vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_train_matrix_80k = create_data_matrix(sample_X_train, max_vocab=80000, word_index=word_index_80k)\n",
    "sample_test_matrix_80k = create_data_matrix(sample_X_test, max_vocab=80000, word_index=word_index_80k)\n",
    "\n",
    "embedding_matrix_100_80k = create_embedding_matrix(max_vocab=80000, word_index=word_index_80k)\n",
    "embedding_matrix_200_80k = create_embedding_matrix(max_vocab=80000, embeddings=embeddings_200,\n",
    "                                                   word_index=word_index_80k, embedding_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 335s 42ms/step - loss: 0.6531 - acc: 0.6055 - val_loss: 0.5904 - val_acc: 0.6810\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 331s 41ms/step - loss: 0.5496 - acc: 0.7157 - val_loss: 0.5283 - val_acc: 0.7225\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 326s 41ms/step - loss: 0.5177 - acc: 0.7329 - val_loss: 0.5201 - val_acc: 0.7270\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 319s 40ms/step - loss: 0.5090 - acc: 0.7362 - val_loss: 0.5082 - val_acc: 0.7340\n"
     ]
    }
   ],
   "source": [
    "#  Try first with the 100-dimension embedding\n",
    "run_trial(embedding_matrix=embedding_matrix_100_80k, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 340s 42ms/step - loss: 0.6315 - acc: 0.6401 - val_loss: 0.5843 - val_acc: 0.6650\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 337s 42ms/step - loss: 0.5360 - acc: 0.7154 - val_loss: 0.5298 - val_acc: 0.7270\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 335s 42ms/step - loss: 0.5027 - acc: 0.7434 - val_loss: 0.4996 - val_acc: 0.7500\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 340s 43ms/step - loss: 0.4912 - acc: 0.7468 - val_loss: 0.4951 - val_acc: 0.7450\n"
     ]
    }
   ],
   "source": [
    "run_trial(embedding_matrix=embedding_matrix_200_80k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 522s 65ms/step - loss: 0.6212 - acc: 0.6391 - val_loss: 0.5885 - val_acc: 0.6900\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 520s 65ms/step - loss: 0.5325 - acc: 0.7174 - val_loss: 0.5203 - val_acc: 0.7265\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 517s 65ms/step - loss: 0.5031 - acc: 0.7429 - val_loss: 0.5181 - val_acc: 0.7245\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 518s 65ms/step - loss: 0.4919 - acc: 0.7490 - val_loss: 0.4955 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200_80k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 520s 65ms/step - loss: 0.6622 - acc: 0.5958 - val_loss: 0.5980 - val_acc: 0.6670\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 522s 65ms/step - loss: 0.5414 - acc: 0.7175 - val_loss: 0.5156 - val_acc: 0.7345\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 519s 65ms/step - loss: 0.5050 - acc: 0.7386 - val_loss: 0.5086 - val_acc: 0.7470\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 517s 65ms/step - loss: 0.4950 - acc: 0.7404 - val_loss: 0.4970 - val_acc: 0.7465\n"
     ]
    }
   ],
   "source": [
    "# Raise the base learning rate a little\n",
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200_80k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k, base_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Try increasing the vocab to 150,000\n",
    "word_index_150k = {}\n",
    "for ix, (word, _) in enumerate(words.most_common(150000)):\n",
    "    word_index_150k[word] = ix + 1 # The zero index is reserved for masking out-of-vocab words\n",
    "    \n",
    "sample_train_matrix_150k = create_data_matrix(sample_X_train, max_vocab=150000, word_index=word_index_150k)\n",
    "sample_test_matrix_150k = create_data_matrix(sample_X_test, max_vocab=150000, word_index=word_index_150k)\n",
    "\n",
    "embedding_matrix_100_150k = create_embedding_matrix(max_vocab=150000, word_index=word_index_150k)\n",
    "embedding_matrix_200_150k = create_embedding_matrix(max_vocab=150000, embeddings=embeddings_200,\n",
    "                                                    word_index=word_index_150k, embedding_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 318s 40ms/step - loss: 0.6479 - acc: 0.6171 - val_loss: 0.5887 - val_acc: 0.6940\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 316s 39ms/step - loss: 0.5557 - acc: 0.7161 - val_loss: 0.5293 - val_acc: 0.7160\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 318s 40ms/step - loss: 0.5184 - acc: 0.7275 - val_loss: 0.5126 - val_acc: 0.7420\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 316s 40ms/step - loss: 0.5067 - acc: 0.7366 - val_loss: 0.5215 - val_acc: 0.7375\n"
     ]
    }
   ],
   "source": [
    "run_trial(embedding_matrix=embedding_matrix_100_150k, cyclic=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 309s 39ms/step - loss: 0.6428 - acc: 0.6240 - val_loss: 0.5776 - val_acc: 0.7120\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 307s 38ms/step - loss: 0.5404 - acc: 0.7200 - val_loss: 0.5143 - val_acc: 0.7330\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 308s 38ms/step - loss: 0.5076 - acc: 0.7361 - val_loss: 0.5137 - val_acc: 0.7300\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 309s 39ms/step - loss: 0.4993 - acc: 0.7463 - val_loss: 0.5074 - val_acc: 0.7395\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=128, attention_dim=32, embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 485s 61ms/step - loss: 0.6460 - acc: 0.6213 - val_loss: 0.6194 - val_acc: 0.6580\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 486s 61ms/step - loss: 0.5470 - acc: 0.7159 - val_loss: 0.5198 - val_acc: 0.7335\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 482s 60ms/step - loss: 0.5077 - acc: 0.7409 - val_loss: 0.5033 - val_acc: 0.7480\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 484s 61ms/step - loss: 0.4944 - acc: 0.7519 - val_loss: 0.5017 - val_acc: 0.7540\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, cyclic=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with GRU/attention dimensions of 200/100 together with 200-dimension embeddings performed best, with very little difference between the scores for the 50k and 150k vocabs. The former produced better scores, but the latter was more consistent with monotonically decreasing loss and increasing accuracy. Neither, however, betters the results woith the adam optimizer, which needs to be tried with the larger vocabs and higher-dimension embeddings. Earlier experiments showed that increasing the dropout rate to 60% helps with overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 495s 62ms/step - loss: 0.5797 - acc: 0.6763 - val_loss: 0.5154 - val_acc: 0.7315\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 485s 61ms/step - loss: 0.4925 - acc: 0.7474 - val_loss: 0.4669 - val_acc: 0.7835\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 486s 61ms/step - loss: 0.4563 - acc: 0.7762 - val_loss: 0.4881 - val_acc: 0.7675\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 484s 61ms/step - loss: 0.4159 - acc: 0.8030 - val_loss: 0.4560 - val_acc: 0.7720\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200_80k, embedding_dim=200, adam=True,\n",
    "          X=sample_train_matrix_80k, X_val=sample_test_matrix_80k, word_index=word_index_80k, drop_pct=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 316s 39ms/step - loss: 0.5779 - acc: 0.6796 - val_loss: 0.4887 - val_acc: 0.7560\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 310s 39ms/step - loss: 0.4914 - acc: 0.7578 - val_loss: 0.5462 - val_acc: 0.6965\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 309s 39ms/step - loss: 0.4561 - acc: 0.7768 - val_loss: 0.4610 - val_acc: 0.7780\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 310s 39ms/step - loss: 0.4259 - acc: 0.7964 - val_loss: 0.4386 - val_acc: 0.7935\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=128, attention_dim=32, embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, adam=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k, drop_pct=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 317s 40ms/step - loss: 0.5796 - acc: 0.6796 - val_loss: 0.4830 - val_acc: 0.7665\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 317s 40ms/step - loss: 0.4826 - acc: 0.7636 - val_loss: 0.4650 - val_acc: 0.7760\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 320s 40ms/step - loss: 0.4433 - acc: 0.7869 - val_loss: 0.4583 - val_acc: 0.7890\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 319s 40ms/step - loss: 0.4237 - acc: 0.8007 - val_loss: 0.4388 - val_acc: 0.7955\n"
     ]
    }
   ],
   "source": [
    "run_trial(embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, adam=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k, drop_pct=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 496s 62ms/step - loss: 0.5740 - acc: 0.6814 - val_loss: 0.5078 - val_acc: 0.7325\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 487s 61ms/step - loss: 0.4811 - acc: 0.7524 - val_loss: 0.4803 - val_acc: 0.7605\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 487s 61ms/step - loss: 0.4461 - acc: 0.7826 - val_loss: 0.5100 - val_acc: 0.7330\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 487s 61ms/step - loss: 0.4213 - acc: 0.7963 - val_loss: 0.4614 - val_acc: 0.7770\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=200, attention_dim=100, embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, adam=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k, drop_pct=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adam optimizer consistently outperforms SGD with a cyclic learning rate and is significantly quicker. The best performer in terms of scores and consistency was with a 128/64 attention network, the 150k vocab, 200-dimensional embedding vectors and 60% dropout. But the much more compact and less computationally expensive 32/16 network with the 50k vocab, 100-dimension embeddings and 50% dropout performed almost as well. Try one more trial of the larger network with 50% dropout to see if the scores improve without significant overfitting, as well as the very compact 64/32 and 32/16 attention networks that worked well on the smaller vocab and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 355s 44ms/step - loss: 0.5815 - acc: 0.6750 - val_loss: 0.5445 - val_acc: 0.6955\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 347s 43ms/step - loss: 0.4913 - acc: 0.7499 - val_loss: 0.4951 - val_acc: 0.7510\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 347s 43ms/step - loss: 0.4522 - acc: 0.7785 - val_loss: 0.4737 - val_acc: 0.7650\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 342s 43ms/step - loss: 0.4176 - acc: 0.8016 - val_loss: 0.4470 - val_acc: 0.7855\n"
     ]
    }
   ],
   "source": [
    "run_trial(embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, adam=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 207s 26ms/step - loss: 0.5865 - acc: 0.6719 - val_loss: 0.5021 - val_acc: 0.7480\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 193s 24ms/step - loss: 0.4956 - acc: 0.7470 - val_loss: 0.4566 - val_acc: 0.7825\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 189s 24ms/step - loss: 0.4458 - acc: 0.7855 - val_loss: 0.4552 - val_acc: 0.7790\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 188s 24ms/step - loss: 0.4284 - acc: 0.7923 - val_loss: 0.4638 - val_acc: 0.7660\n"
     ]
    }
   ],
   "source": [
    "run_trial(GRU_dim=64, attention_dim=32, embedding_matrix=embedding_matrix_200_150k, embedding_dim=200, adam=True,\n",
    "          X=sample_train_matrix_150k, X_val=sample_test_matrix_150k, word_index=word_index_150k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither the smaller dropout rate nor the smaller network dimensions increased performance. Runs of the two models specified above will be tried with the full datasets on AWS Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
