{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Embedding, GRU, LSTM, Bidirectional, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cleaned/cred_sample', 'rb') as infile:\n",
    "    cred = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cleaned/hate', 'rb') as infile:\n",
    "    hate = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hate) > len(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hate = hate.sample(len(cred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hate) == len(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hate['label'] = 0\n",
    "cred['label'] = 1\n",
    "data = cred.append(hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 30  # max num words processed for each sentence\n",
    "max_sentences = 30  # max num sentences processed for each article \n",
    "max_vocab = 50001  # size most-frequent vocab including 0 index for any out-of-vocab word\n",
    "embedding_dim = 100  # size of pretrained word vectors\n",
    "attention_dim = 128  # num units in attention layer\n",
    "batch_size = 128\n",
    "test_val_size = 0.2\n",
    "articles = []\n",
    "texts = []\n",
    "embeddings = {}\n",
    "\n",
    "vector_dir = './embeddings'\n",
    "vector_file = 'glove.6B.100d.txt'\n",
    "model_dir = './model_output/glove_100'\n",
    "tb_logs = './tb_logs/glove_100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import matmul\n",
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    ''''''\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init_weights = initializers.get('glorot_normal')\n",
    "        self.init_bias = initializers.get('zeros')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(HierarchicalAttentionNetwork, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init_weights((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init_bias((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init_weights((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(HierarchicalAttentionNetwork, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x: [batch_size, max_words/max_sentences, attention_dim]\n",
    "        # size of u: [batch_size, attention_dim]\n",
    "        # uit = tanh(Wx + b)\n",
    "        # ait = softmax(uit*u)\n",
    "        \n",
    "        #uit = K.tile(K.expand_dims(self.W, axis=0), (K.shape(x)[0], 1, 1))\n",
    "        #uit = matmul(x, uit)\n",
    "        #uit = K.tanh(K.bias_add(uit, self.b))\n",
    "        #ait = K.dot(uit, self.u)\n",
    "        #ait = K.squeeze(ait, -1)\n",
    "\n",
    "        #ait = K.exp(ait)\n",
    "        \n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['content'], data['label'], test_size=test_val_size,\n",
    "                                                    random_state=19, stratify=data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = np.asarray(y_train), np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "for text in data['content']:\n",
    "    texts.append(text)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenizer.num_words = max_vocab\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80473"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def sentencize(article):\n",
    "    return [sent for sent in nlp(article).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.apply(sentencize)\n",
    "x_test = x_test.apply(sentencize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_punct(article):\n",
    "    stripped = []\n",
    "    for sent in enumerate(article):\n",
    "        stripped.append([token.text.lower() for token in sent[1] if token.pos_ !='PUNCT'])\n",
    "    return stripped  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.apply(no_punct)\n",
    "x_test = x_test.apply(no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_matrix = np.zeros((len(x_train), max_sentences, max_words), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentences in enumerate(x_train):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_sentences:\n",
    "            k = 0\n",
    "            for _, word in enumerate(sent):\n",
    "                if k < max_words:\n",
    "                    ix = word_index.get(word.lower())\n",
    "                    if ix is not None and ix < max_vocab:\n",
    "                        data_matrix[i, j, k] = ix\n",
    "                        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_matrix = np.zeros((len(x_test), max_sentences, max_words), dtype='int32')\n",
    "for i, sentences in enumerate(x_test):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_sentences:\n",
    "            k = 0\n",
    "            for _, word in enumerate(sent):\n",
    "                if k < max_words:\n",
    "                    ix = word_index.get(word.lower())\n",
    "                    if ix is not None and ix < max_vocab:\n",
    "                        test_matrix[i, j, k] = ix\n",
    "                        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(vector_dir, vector_file)) as vectors:\n",
    "    for line in vectors:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        weights = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "\n",
    "# embedding layer\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                            input_length=max_words, trainable=True, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers for processing words in each sentence with attention; output is encoded sentence vector \n",
    "\n",
    "sentence_input = Input(shape=(max_words,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "lstm_word = Bidirectional(GRU(128, return_sequences=True))(embedded_sequences)\n",
    "attn_word = HierarchicalAttentionNetwork(128)(lstm_word)\n",
    "sentence_encoder = Model(sentence_input, attn_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layers for processing sentences in each article with attention; output is prediction\n",
    "\n",
    "article_input = Input(shape=(max_sentences, max_words), dtype='int32')\n",
    "article_encoder = TimeDistributed(sentence_encoder)(article_input)\n",
    "lstm_sentence = Bidirectional(GRU(128, return_sequences=True))(article_encoder)\n",
    "attn_sentence = HierarchicalAttentionNetwork(128)(lstm_sentence)\n",
    "preds = Dense(2, activation='softmax')(attn_sentence)\n",
    "model = Model(article_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create checkpoints to save information from each training epoch\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=model_dir+'weights.{epoch:02d}.hdf5')\n",
    "tb_checkpoint = TensorBoard(log_dir=tb_logs, histogram_freq=1, batch_size=128, write_graph=False, write_grads=True,\n",
    "                            write_images=True)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "if not os.path.exists(tb_logs):\n",
    "        os.makedirs(tb_logs, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13043 samples, validate on 3261 samples\n",
      "Epoch 1/4\n",
      "13043/13043 [==============================] - 631s 48ms/step - loss: 0.6388 - acc: 0.6342 - val_loss: 0.5097 - val_acc: 0.7590\n",
      "Epoch 2/4\n",
      "13043/13043 [==============================] - 547s 42ms/step - loss: 0.3984 - acc: 0.8198 - val_loss: 0.3202 - val_acc: 0.8654\n",
      "Epoch 3/4\n",
      "13043/13043 [==============================] - 555s 43ms/step - loss: 0.2348 - acc: 0.9035 - val_loss: 0.2853 - val_acc: 0.8826\n",
      "Epoch 4/4\n",
      "13043/13043 [==============================] - 554s 42ms/step - loss: 0.1397 - acc: 0.9485 - val_loss: 0.2727 - val_acc: 0.8960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee143e748>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_matrix, y_train, validation_data=(test_matrix, y_test), epochs=4, batch_size=128, callbacks=[model_checkpoint, tb_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(np.asarray(pd.DataFrame(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(np.asarray(pd.DataFrame(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13043, 2)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13043,)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      multiple                  8047400   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 30, 256)           175872    \n",
      "_________________________________________________________________\n",
      "hierarchical_attention_netwo (None, 256)               33024     \n",
      "=================================================================\n",
      "Total params: 8,256,296\n",
      "Trainable params: 8,256,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 30, 30)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 30, 256)           8256296   \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 30, 256)           295680    \n",
      "_________________________________________________________________\n",
      "hierarchical_attention_netwo (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 8,585,514\n",
      "Trainable params: 8,585,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13043, 30, 30)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   54,   907,    51,   211,    17,    80,   257,   117,   169,\n",
       "           20,     8,   822,     2,  2855,    39,  1292,     8,     1,\n",
       "         1435,   114,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [  262,   625,    34,     6,   474,     2,   591,     4,   857,\n",
       "          584,     3,    26,  3921,    48,  4239,    62,     6,   613,\n",
       "          474,     2,   877,     4,  1195,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [ 2097,     3,     5,     4,   167,    24,     1,  1292,    12,\n",
       "           24,   767,    15,  3825,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   14,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   33,    50,    26,     1,  2772,   474,     8,     1,  1435,\n",
       "          114,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   66,  2641,    39,   857,   584,     8,   211,  1101,    33,\n",
       "          625,  4849,    61,     1, 13563,    27,     1,  1800,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   23, 11031,     1, 22038,   670,   430,  6357,    27,     1,\n",
       "          973,  9145,  7604,     2,    14,     3,    14,    57,     2,\n",
       "           14,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   29,  2000,    13,  1170,   563,    59,    23,    83,   880,\n",
       "            6,   273,     2,    86, 10153,  6357,    11,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   23,    34,   228,     3,    66,  1195,     4,    29,   625,\n",
       "           13,  1170, 19253,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [    1, 17245,   583,    15,    33,   625,    13,   150,   196,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   14,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   33,    13,     1,   250,   474,    61,     8,     1,  1435,\n",
       "          114,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [ 1081,     6,   228,  2641,    39,   857,   584,     8,   211,\n",
       "           32,    23,   137,   880,     1,  2712,   149,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   23,    67,  1272,    61,  6357,    27,     1,   973,  9145,\n",
       "         7604,     2,    14,     3,    14,    57,     2,    14,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   29,  2000,    13,  1170,   563,     4,    23,  4607,    66,\n",
       "         1195,   120,    23,  1975,    29,  2935,     8,   211,    19,\n",
       "            6,  2719,   355,     3,     5,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   23,    13, 19253,   248,    97,    47,  2966,     3,    91,\n",
       "         1198,    61,    16,     1,  9087,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [    1, 17245,   583,    15,    33,   625,    13,   150,   196,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   23,  2382,   333,   248,   144,   438,    16,     1,   496,\n",
       "            3,   124,  2151,   866,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   14,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   33,    13,     1,   487,   474,     8,     1,  1435,   114,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   23,    34,    71,  2641,     4,   857,   584,    17,   211,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   33,   115,    13, 13639,    17,     1,  3361,  4309,     2,\n",
       "         2517,    23,  8807,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   23,   801,   152,  2973,    59,    77,   222,    29,   877,\n",
       "        31963,    23,  5897,     3,    28,     6,  1170,   163,    11,\n",
       "           39,    11,  1211,  2521,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [    9,    16,     1,    65,   507,    34,    46,  1214,  1980,\n",
       "           12,    23,    13,     6,   437,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   23,    34,    46, 20285,    61,    36,    45,    44,   196,\n",
       "            8,   846,     4,    86,  1457,    28,    46,  2435,    91,\n",
       "            3,    65,  1457,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [    9,    34,    46,   802,    15,     6,  4031,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   14,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   33,    13,     1,  1806,   474,     8,     1,  1435,   114,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [   33,   625,   171,    28,     6,   163,   474,     2,  2641,\n",
       "            4,   857,   584,    17,   211,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [  512,  3956,  3208,  3131,  1830,    67,  3068,     6, 14443,\n",
       "            2,    80,     8,   163,   973,  2483,     3,  3612,  9795,\n",
       "        11327,  8415,  4343,     3,  1096,     4,   863,   824,    16,\n",
       "           44,     4,  9475]], dtype=int32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
