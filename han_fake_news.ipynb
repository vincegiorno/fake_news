{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Embedding, GRU, LSTM, Bidirectional, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cleaned/cred_sample', 'rb') as infile:\n",
    "    cred = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cleaned/hate', 'rb') as infile:\n",
    "    hate = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hate) > len(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hate = hate.sample(len(cred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hate) == len(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hate['label'] = 0\n",
    "cred['label'] = 1\n",
    "data = cred.append(hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 30\n",
    "max_sentences = 30\n",
    "max_vocab = 50001\n",
    "embedding_dim = 100\n",
    "attention_dim = 128\n",
    "test_val_size = 0.2\n",
    "articles = []\n",
    "texts = []\n",
    "embeddings = {}\n",
    "\n",
    "vector_dir = './embeddings'\n",
    "vector_file = 'glove.6B.100d.txt'\n",
    "model_dir = './model_output/glove_100'\n",
    "tb_logs = './tb_logs/glove_100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    ''''''\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init_weights = initializers.get('glorot_normal')\n",
    "        self.init_bias = initializers.get('zeros')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(HierarchicalAttentionNetwork, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init_weights((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init_bias((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init_weights((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(HierarchicalAttentionNetwork, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x: [batch_size, max_words/max_sentences, attention_dim]\n",
    "        # size of u: [batch_size, attention_dim]\n",
    "        # uit = tanh(Wx + b)\n",
    "        # ait = softmax(uit*u)\n",
    "        \n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['content'], data['label'], test_size=test_val_size,\n",
    "                                                    random_state=19, stratify=data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "for text in data['content']:\n",
    "    texts.append(text)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenizer.num_words = max_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80473"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def sentencize(article):\n",
    "    return [sent for sent in nlp(article).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.apply(sentencize)\n",
    "x_test = x_test.apply(sentencize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_punct(article):\n",
    "    stripped = []\n",
    "    for sent in enumerate(article):\n",
    "        stripped.append([token.text.lower() for token in sent[1] if token.pos_ !='PUNCT'])\n",
    "    return stripped  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.apply(no_punct)\n",
    "x_test = x_test.apply(no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_matrix = np.zeros((len(x_train), max_sentences, max_words), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentences in enumerate(x_train):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_sentences:\n",
    "            k = 0\n",
    "            for _, word in enumerate(sent):\n",
    "                if k < max_words:\n",
    "                    ix = tokenizer.word_index.get(word.lower())\n",
    "                    if ix is not None and ix < max_vocab:\n",
    "                        data_matrix[i, j, k] = ix\n",
    "                        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(filepath=model_dir+'weights.{epoch:02d}.hdf5')\n",
    "tb_checkpoint = TensorBoard(log_dir=tb_logs, histogram_freq=1, batch_size=128, write_graph=False, write_grads=True,\n",
    "                            write_images=True)\n",
    "\n",
    "# callbacks=[model_checkpoint, tb_checkpoint]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
