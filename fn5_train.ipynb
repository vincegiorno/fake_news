{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import utils.config as config\n",
    "from cyclic.rate_cycler import CyclicLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.layers import Embedding, GRU, LSTM, Bidirectional, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, Callback, LambdaCallback\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reliable = pd.read_csv('data_final/reliable_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "set_random_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_final/reliable_final.pkl', 'rb') as infile:\n",
    "    reliable = pickle.load(infile)\n",
    "    \n",
    "with open('data_final/unreliable_final.pkl', 'rb') as infile:\n",
    "    unreliable = pickle.load(infile)\n",
    "    \n",
    "reliable['label'] = 0\n",
    "unreliable['label'] = 1\n",
    "data = reliable.append(unreliable).sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = config.max_words  # max num words processed for each sentence\n",
    "max_sentences = config.max_sentences  # max num sentences processed for each article \n",
    "max_vocab = config.max_vocab\n",
    "embedding_dim = config.embedding_dim  # size of pretrained word vectors\n",
    "attention_dim = config.attention_dim  # num units in attention layer\n",
    "GRU_dim = config.GRU_dim  # num units in GRU layer, but it is bidirectional so outputs double this number\n",
    "epochs = config.epochs\n",
    "batch_size = config.batch_size\n",
    "test_size = config.test_size\n",
    "\n",
    "vector_file = 'embeddings/glove.6B.200d.txt'\n",
    "model_dir = 'model_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets. Test set will not be used in any way until model is trained.\n",
    "\n",
    "data_train, X_test, labels_train, y_test = train_test_split(data['content'], data['label'], test_size=test_size,\n",
    "                                                    random_state=77, stratify=data['label'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_train, labels_train, test_size=test_size,\n",
    "                                                    random_state=77, stratify=labels_train)\n",
    "y_train = np.asarray(to_categorical(y_train))\n",
    "y_val = np.asarray(to_categorical(y_val))\n",
    "y_test = np.asarray(to_categorical(y_test))\n",
    "\n",
    "num_samples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_final/words.pkl', 'rb') as infile:\n",
    "    words = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "for ix, (word, _) in enumerate(words.most_common(max_vocab)):\n",
    "    word_index[word] = ix + 1 # The zero index is reserved for masking out-of-vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_matrix(data, max_sentences=max_sentences, max_words=max_words, max_vocab=max_vocab,\n",
    "                      word_index=word_index):\n",
    "    data_matrix = np.zeros((len(data), max_sentences, max_words), dtype='int32')\n",
    "    for i, article in enumerate(data):\n",
    "        for j, sentence in enumerate(article):\n",
    "            if j == max_sentences:\n",
    "                break\n",
    "            k = 0\n",
    "            for word in sentence:\n",
    "                if k == max_words:\n",
    "                    break\n",
    "                ix = word_index.get(word.lower())\n",
    "                if ix is not None and ix < max_vocab:\n",
    "                    data_matrix[i, j, k] = ix\n",
    "                k = k + 1\n",
    "    return data_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = create_data_matrix(X_train)\n",
    "val_matrix = create_data_matrix(X_val)\n",
    "test_matrix = create_data_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_embeddings(vector_file=vector_file):\n",
    "    embeddings = {}\n",
    "    with open(vector_file) as vectors:\n",
    "        for line in vectors:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            weights = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = weights\n",
    "    return embeddings\n",
    "            \n",
    "embeddings = store_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(max_vocab=max_vocab, embeddings=embeddings, word_index=word_index,\n",
    "                            embedding_dim=embedding_dim):\n",
    "    embedding_matrix = np.zeros((max_vocab + 1, embedding_dim)) # max_vocab + 1 to account for 0 as masking index\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will remain all zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "            \n",
    "embedding_matrix = create_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct attention layer\n",
    "\n",
    "from tensorflow import matmul\n",
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    ''''''\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init_weights = initializers.get('glorot_normal')\n",
    "        self.init_bias = initializers.get('zeros')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init_weights((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init_bias((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init_weights((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):        \n",
    "        #uit = K.tile(K.expand_dims(self.W, axis=0), (K.shape(x)[0], 1, 1))\n",
    "        #uit = matmul(x, uit)\n",
    "        #uit = K.tanh(K.bias_add(uit, self.b))\n",
    "        #ait = K.dot(uit, self.u)\n",
    "        #ait = K.squeeze(ait, -1)\n",
    "\n",
    "        #ait = K.exp(ait)\n",
    "        \n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(attention_dim=attention_dim, GRU_dim=GRU_dim, drop=False, drop_pct=None,\n",
    "                embedding_matrix=embedding_matrix, embedding_dim=embedding_dim, word_index=word_index):\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                                input_length=max_words, trainable=False, mask_zero=True)\n",
    "\n",
    "    #  Layers for processing words in each sentence with attention; output is encoded sentence vector \n",
    "    sentence_input = Input(shape=(max_words,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    lstm_word = Bidirectional(GRU(GRU_dim, return_sequences=True))(embedded_sequences)\n",
    "    attn_word = HierarchicalAttentionNetwork(attention_dim)(lstm_word)\n",
    "    sentence_encoder = Model(sentence_input, attn_word)\n",
    "    \n",
    "    #  Layers for processing sentences in each article with attention; output is prediction\n",
    "    article_input = Input(shape=(max_sentences, max_words), dtype='int32')\n",
    "    article_encoder = TimeDistributed(sentence_encoder)(article_input)\n",
    "    lstm_sentence = Bidirectional(GRU(GRU_dim, return_sequences=True))(article_encoder)\n",
    "    attn_sentence = HierarchicalAttentionNetwork(attention_dim)(lstm_sentence)\n",
    "    #  The Adam optimizer also will be tried and can take a dropout layer\n",
    "    if drop:\n",
    "        drop_sentence = Dropout(drop_pct)(attn_sentence)\n",
    "        preds = Dense(2, activation='softmax')(drop_sentence)\n",
    "    else:\n",
    "        preds = Dense(2, activation='softmax')(attn_sentence)\n",
    "    \n",
    "    return Model(article_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create checkpoints to save information from each training epoch\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='model_output/weights.{epoch:02d}-{val_loss:.2f}.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CyclicLR class defaults to the min and max learning rates determined previously, $10^{-2.5}$ and $10^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clr = CyclicLR(epochs=epochs, num_samples=num_samples, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "model = build_model()\n",
    "opt = SGD(momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "\n",
    "model.fit(train_matrix, y_train, validation_data=(val_matrix, y_val),\n",
    "          batch_size=batch_size, epochs=epochs, callbacks=[clr, model_checkpoint])\n",
    "\n",
    "model.save('model_output/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr.plot_lr()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
