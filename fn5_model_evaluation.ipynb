{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from tensorflow import matmul\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_final/aws_data/X_train.pkl', 'rb') as infile:\n",
    "    X_train = pickle.load(infile)\n",
    "    \n",
    "with open('data_final/aws_data/X_val.pkl', 'rb') as infile:\n",
    "    X_val = pickle.load(infile)\n",
    "\n",
    "with open('data_final/aws_data/X_test.pkl', 'rb') as infile:\n",
    "    X_test = pickle.load(infile)\n",
    "\n",
    "with open('data_final/aws_data/y_train.pkl', 'rb') as infile:\n",
    "    y_train = pickle.load(infile)\n",
    "    \n",
    "with open('data_final/aws_data/y_val.pkl', 'rb') as infile:\n",
    "    y_val = pickle.load(infile)\n",
    "\n",
    "with open('data_final/aws_data/y_test.pkl', 'rb') as infile:\n",
    "    y_test = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recombine(array):\n",
    "    '''\n",
    "    Rejoins the lists of words in the articles pre-formatted for training into a single string.\n",
    "    \n",
    "    Returns: String containing all the words in an article that was pre-formatted.\n",
    "    '''\n",
    "    return [' '.join(' '.join(sent) for sent in array)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = X_val.apply(recombine)\n",
    "data = data.append(X_train.apply(recombine))\n",
    "labels = y_val.append(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_data = data.sample(20000)\n",
    "sample_indices = sample_data.index\n",
    "sample_labels = labels[sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(decode_error='ignore', strip_accents='unicode', max_features=50000)\n",
    "counter.fit(data)\n",
    "sample_data_counts = counter.transform(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.16227766])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs = list(np.logspace(-4, 1, num=11))\n",
    "clf = LogisticRegressionCV(Cs=Cs, solver='sag', cv=5)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(data_counts)\n",
    "lrcv = clf.fit(X=tfidf.transform(sample_data_counts), y=sample_labels)\n",
    "lrcv.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Since the best value determined for C was the greatest of the values tried, run again with larger values\n",
    "np.random.seed(3)\n",
    "Cs = list(np.linspace(1, 10, num=10))\n",
    "clf = LogisticRegressionCV(Cs=Cs, solver='sag', cv=5)\n",
    "lrcv = clf.fit(X=tfidf.fit_transform(sample_data_counts), y=sample_labels)\n",
    "lrcv.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vince/anaconda/envs/thinkful/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.]), array([ 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs = list(np.linspace(1, 5, num=5))\n",
    "ratios = list(np.linspace(0, 1, num=5))\n",
    "clf = LogisticRegressionCV(Cs=Cs, l1_ratios=ratios, penalty='elasticnet', solver='saga', cv=5, max_iter=300)\n",
    "lrcv = clf.fit(X=tfidf.transform(sample_data_counts), y=sample_labels)\n",
    "lrcv.C_, lrcv.l1_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression using either the sag or sag solver, a C value of 4 and the L2 penalty produce the optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = X_test.apply(recombine)\n",
    "X_test_counts = counter.transform(test_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86896874999999996"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_counts = counter.transform(data)\n",
    "lr = LogisticRegression(C=4.0, solver='sag', random_state=77, max_iter=300)\n",
    "lr.fit(X=tfidf.transform(data_counts), y=labels)\n",
    "lr.score(X=tfidf.transform(X_test_counts), y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86896093750000003"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=4.0, solver='saga', random_state=77, max_iter=300)\n",
    "lr.fit(X=tfidf.transform(data_counts), y=labels)\n",
    "lr.score(X=tfidf.transform(X_test_counts), y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = CountVectorizer(decode_error='ignore', strip_accents='unicode', max_features=150000)\n",
    "counter.fit(data)\n",
    "data_counts = counter.fit_transform(data)\n",
    "X_test_counts = counter.transform(test_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87283593749999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=4.0, solver='sag', random_state=77, max_iter=300)\n",
    "lr.fit(X=tfidf.fit_transform(data_counts), y=labels)\n",
    "lr.score(X=tfidf.transform(X_test_counts), y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression after tuning for the value of C and either the L1 (only possiblefor the saga solver) or L2 penalty and with a 150,000-word vocabulary did slightly better than the simple cross-validation-fit accuracy of about 0.845 obtained as the benchmark in the data exploration notebook using a vocabulary of less than 5,000 words. Nonetheless, this is an impressive result compared to the HAN model trained using a cyclic learning rate, which achieved a slightly lower validation accuracy. The HAN model trained for 16 hours compared with 5 minutes or so for the logistic regression classifier. The HAN models using the adam optimizer, however, did obtain significantly higher validation scores just over 0.90. But these trained models need to be run on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 30  # max num words processed for each sentence\n",
    "max_sentences = 30  # max num sentences processed for each article \n",
    "max_vocab = 150000\n",
    "attention_dim = 100\n",
    "batch_size = 64\n",
    "words_file = 'data_final/words.pkl'\n",
    "saved_model = 'models/adam-150-200-100/model.4.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(words_file, 'rb') as infile:\n",
    "    words = pickle.load(infile)\n",
    "word_index = {}\n",
    "for ix, (word, _) in enumerate(words.most_common(max_vocab)):\n",
    "    word_index[word] = ix + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_matrix(data, max_sentences=max_sentences, max_words=max_words, max_vocab=max_vocab,\n",
    "                      word_index=word_index):\n",
    "    data_matrix = np.zeros((len(data), max_sentences, max_words), dtype='int32')\n",
    "    for i, article in enumerate(data):\n",
    "        for j, sentence in enumerate(article):\n",
    "            if j == max_sentences:\n",
    "                break\n",
    "            k = 0\n",
    "            for word in sentence:\n",
    "                if k == max_words:\n",
    "                    break\n",
    "                ix = word_index.get(word.lower())\n",
    "                if ix is not None and ix < max_vocab:\n",
    "                    data_matrix[i, j, k] = ix\n",
    "                k = k + 1\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_keras = create_data_matrix(X_test)\n",
    "y_test_keras = np.asarray(to_categorical(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    ''''''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init_weights = initializers.get('glorot_normal')\n",
    "        self.init_bias = initializers.get('zeros')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init_weights((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init_bias((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init_weights((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):        \n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with CustomObjectScope({'HierarchicalAttentionNetwork': HierarchicalAttentionNetwork}):\n",
    "            model = load_model(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Save logistic regression model artifacts and garbage collect to free up memory\n",
    "with open('models/logistic/counter.pkl', 'wb') as outfile:\n",
    "    pickle.dump(counter, outfile)\n",
    "with open('models/logistic/tfidf.pkl', 'wb') as outfile:\n",
    "    pickle.dump(tfidf, outfile)\n",
    "with open('models/logistic/lr.pkl', 'wb') as outfile:\n",
    "    pickle.dump(lr, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "data = None\n",
    "data_counts = None\n",
    "sample_data = None\n",
    "sample_data_counts = None\n",
    "X_train = None\n",
    "X_val = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000/128000 [==============================] - 2970s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22986735509708522, 0.91568749999999999]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_test_keras, y=y_test_keras, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.predict(X_test_keras[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.996813  ,  0.99338996], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = lr.predict_proba(tfidf.transform(X_test_counts[1:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98672044,  0.98276808])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(score[:,1] + scores[:,1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
